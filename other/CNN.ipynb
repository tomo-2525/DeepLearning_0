{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 畳み込みニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) #(9, 75)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape) #(90, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T#フィルターの展開\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layers import *\n",
    "\n",
    "from collections import OrderedDict\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}, hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))#大きさを半分にするプーリングを想定？\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "            \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "            \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "                \n",
    "        grads={}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "        \n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300118257851044\n",
      "=== epoch:1, train acc:0.087, test acc:0.089 ===\n",
      "train loss:2.2988220359130755\n",
      "train loss:2.295447318380627\n",
      "train loss:2.290871043798905\n",
      "train loss:2.2860536827346083\n",
      "train loss:2.2719266760963865\n",
      "train loss:2.2633038365566307\n",
      "train loss:2.2341930137051484\n",
      "train loss:2.2517179401726115\n",
      "train loss:2.1877753944873755\n",
      "train loss:2.2074775901568087\n",
      "train loss:2.127452513113011\n",
      "train loss:2.1284692716878677\n",
      "train loss:2.12382919172393\n",
      "train loss:2.040466116000816\n",
      "train loss:2.0468897290111676\n",
      "train loss:1.9269709746388517\n",
      "train loss:1.8657272868753991\n",
      "train loss:1.8902392271797641\n",
      "train loss:1.7616097106923783\n",
      "train loss:1.628726778311443\n",
      "train loss:1.556419518632294\n",
      "train loss:1.6172739758860528\n",
      "train loss:1.478723617552702\n",
      "train loss:1.3609264977468962\n",
      "train loss:1.2585286741103408\n",
      "train loss:1.1753985882579974\n",
      "train loss:1.1694251853751318\n",
      "train loss:1.1470417989608281\n",
      "train loss:0.9489517689931622\n",
      "train loss:1.1698178415872216\n",
      "train loss:0.8926890268209867\n",
      "train loss:0.8992193339649696\n",
      "train loss:0.7255350807000492\n",
      "train loss:0.8397271224817721\n",
      "train loss:0.9089398663385613\n",
      "train loss:0.876292954183065\n",
      "train loss:0.8630776811711517\n",
      "train loss:0.8204591663239568\n",
      "train loss:0.6113670359726905\n",
      "train loss:0.6312408785175907\n",
      "train loss:0.5819422879134619\n",
      "train loss:0.8181539330871572\n",
      "train loss:0.6048047578666691\n",
      "train loss:0.6228414642556238\n",
      "train loss:0.3859350173588754\n",
      "train loss:0.4928304739253133\n",
      "train loss:0.6148574857909463\n",
      "train loss:0.7915183159413336\n",
      "train loss:0.55915595955625\n",
      "train loss:0.6237747973992857\n",
      "train loss:0.6820834141194154\n",
      "train loss:0.4440103510610933\n",
      "train loss:0.385214227112011\n",
      "train loss:0.5325764712754514\n",
      "train loss:0.48257897180371856\n",
      "train loss:0.4762079018113136\n",
      "train loss:0.6391208646987805\n",
      "train loss:0.48293534831539736\n",
      "train loss:0.5954841221996399\n",
      "train loss:0.5020884232064248\n",
      "train loss:0.4049172778436312\n",
      "train loss:0.2538136712683605\n",
      "train loss:0.649059062885035\n",
      "train loss:0.29703576319316033\n",
      "train loss:0.43613733267090593\n",
      "train loss:0.46939422843132095\n",
      "train loss:0.4332596082104586\n",
      "train loss:0.2800314358949438\n",
      "train loss:0.42547258644972963\n",
      "train loss:0.5287102910997554\n",
      "train loss:0.34577238913953123\n",
      "train loss:0.5936576702275637\n",
      "train loss:0.5303578420061962\n",
      "train loss:0.4761000626203588\n",
      "train loss:0.6607747911829931\n",
      "train loss:0.4397405235849344\n",
      "train loss:0.3980553240241347\n",
      "train loss:0.6149770815713616\n",
      "train loss:0.3497427518117686\n",
      "train loss:0.30934918068301037\n",
      "train loss:0.49383151332965347\n",
      "train loss:0.45385037451827664\n",
      "train loss:0.4619384590598544\n",
      "train loss:0.43646300235584456\n",
      "train loss:0.3517732646920053\n",
      "train loss:0.3915397780314019\n",
      "train loss:0.42032155964811324\n",
      "train loss:0.3944291905663009\n",
      "train loss:0.38825956969989833\n",
      "train loss:0.3820451314571076\n",
      "train loss:0.5151544922186684\n",
      "train loss:0.5473289536819524\n",
      "train loss:0.3456675820898656\n",
      "train loss:0.3464398112589431\n",
      "train loss:0.5268154003819261\n",
      "train loss:0.40618943688970516\n",
      "train loss:0.38481375974468324\n",
      "train loss:0.46996165790946287\n",
      "train loss:0.41261000326860287\n",
      "train loss:0.48779532592719355\n",
      "train loss:0.46136792690131967\n",
      "train loss:0.3956371445291063\n",
      "train loss:0.4843673753372014\n",
      "train loss:0.43042676176911476\n",
      "train loss:0.34331785150510674\n",
      "train loss:0.4284104518981936\n",
      "train loss:0.3142506275962229\n",
      "train loss:0.41040141706979144\n",
      "train loss:0.3866004236995092\n",
      "train loss:0.2560622323448291\n",
      "train loss:0.5082073195113893\n",
      "train loss:0.42165870930869304\n",
      "train loss:0.26161012990834237\n",
      "train loss:0.4122427717894358\n",
      "train loss:0.3889847266428063\n",
      "train loss:0.33926280806509446\n",
      "train loss:0.3202014682548693\n",
      "train loss:0.3885365184077352\n",
      "train loss:0.3757146467594932\n",
      "train loss:0.27832384622089107\n",
      "train loss:0.4159145996414542\n",
      "train loss:0.2551904695673761\n",
      "train loss:0.3020918258062075\n",
      "train loss:0.28962238675670465\n",
      "train loss:0.30243786921900473\n",
      "train loss:0.20914300126355573\n",
      "train loss:0.3324374448189754\n",
      "train loss:0.36458709522806126\n",
      "train loss:0.3139219122426887\n",
      "train loss:0.348267700909378\n",
      "train loss:0.3079964748347351\n",
      "train loss:0.3572013021546947\n",
      "train loss:0.32214733434391696\n",
      "train loss:0.1991396684487296\n",
      "train loss:0.31466229467923823\n",
      "train loss:0.37464829091829555\n",
      "train loss:0.39155925667942787\n",
      "train loss:0.3249028325145988\n",
      "train loss:0.3065997490275663\n",
      "train loss:0.2357224085270734\n",
      "train loss:0.33827622120124373\n",
      "train loss:0.4779310941650929\n",
      "train loss:0.34949838540033346\n",
      "train loss:0.1442067349562653\n",
      "train loss:0.39472533682902994\n",
      "train loss:0.26640189659235075\n",
      "train loss:0.3551001095991619\n",
      "train loss:0.35583063870639015\n",
      "train loss:0.34706152723745176\n",
      "train loss:0.46918580696652773\n",
      "train loss:0.3528511431820871\n",
      "train loss:0.3055332456092758\n",
      "train loss:0.50853145577962\n",
      "train loss:0.3621998784657812\n",
      "train loss:0.23329026227004884\n",
      "train loss:0.38601134996427283\n",
      "train loss:0.1963269928961011\n",
      "train loss:0.3134083986929005\n",
      "train loss:0.30827034555882177\n",
      "train loss:0.20659925076149363\n",
      "train loss:0.30230057390810045\n",
      "train loss:0.3096477300756512\n",
      "train loss:0.33395796121030535\n",
      "train loss:0.28993469307154807\n",
      "train loss:0.4911841469286341\n",
      "train loss:0.1603859399165033\n",
      "train loss:0.3481652683435775\n",
      "train loss:0.2610161852700981\n",
      "train loss:0.31335896160980714\n",
      "train loss:0.45808033184168534\n",
      "train loss:0.3282122526098118\n",
      "train loss:0.24443299597700346\n",
      "train loss:0.21298657288626413\n",
      "train loss:0.37819177532642134\n",
      "train loss:0.3806224813082182\n",
      "train loss:0.366158563876603\n",
      "train loss:0.35821428155808815\n",
      "train loss:0.23107817841732678\n",
      "train loss:0.43051976501776396\n",
      "train loss:0.24948643590790393\n",
      "train loss:0.4012424641385844\n",
      "train loss:0.30011214597374614\n",
      "train loss:0.15867004451460545\n",
      "train loss:0.228759861788478\n",
      "train loss:0.2616962782123859\n",
      "train loss:0.21706123091829516\n",
      "train loss:0.31392184928194466\n",
      "train loss:0.2305889705197265\n",
      "train loss:0.25131747505283525\n",
      "train loss:0.2989127597317445\n",
      "train loss:0.2449312029750666\n",
      "train loss:0.2835122252415132\n",
      "train loss:0.22016332782642897\n",
      "train loss:0.24184797285011758\n",
      "train loss:0.4832775567668682\n",
      "train loss:0.3054862139314748\n",
      "train loss:0.33500056199814254\n",
      "train loss:0.2725710155205264\n",
      "train loss:0.339668287254206\n",
      "train loss:0.311462632226172\n",
      "train loss:0.21616553162347113\n",
      "train loss:0.25872489100030943\n",
      "train loss:0.1791605276066444\n",
      "train loss:0.40816368478771936\n",
      "train loss:0.25077730144890614\n",
      "train loss:0.31009154426998164\n",
      "train loss:0.33713420365966557\n",
      "train loss:0.27693185461888886\n",
      "train loss:0.3259529081176156\n",
      "train loss:0.47632403162233294\n",
      "train loss:0.2185224667563763\n",
      "train loss:0.3761107948115704\n",
      "train loss:0.3295974391771378\n",
      "train loss:0.3125875112330511\n",
      "train loss:0.16387469376252134\n",
      "train loss:0.3216110426313192\n",
      "train loss:0.22472899031739996\n",
      "train loss:0.3406296158170564\n",
      "train loss:0.34693900389627\n",
      "train loss:0.3077683600650194\n",
      "train loss:0.3501026325166981\n",
      "train loss:0.48514219376707923\n",
      "train loss:0.2573197995010527\n",
      "train loss:0.1522966785033454\n",
      "train loss:0.24673654617156643\n",
      "train loss:0.33426109401044307\n",
      "train loss:0.31424181477323115\n",
      "train loss:0.3445405764146362\n",
      "train loss:0.19388399657089558\n",
      "train loss:0.28719772082519895\n",
      "train loss:0.22787598493297356\n",
      "train loss:0.2728023282572549\n",
      "train loss:0.40666680432927504\n",
      "train loss:0.3551910437314775\n",
      "train loss:0.20943923195554112\n",
      "train loss:0.3000074962824635\n",
      "train loss:0.22850867893362256\n",
      "train loss:0.3576823091967201\n",
      "train loss:0.20846057212439312\n",
      "train loss:0.38751066805178835\n",
      "train loss:0.28419989820177327\n",
      "train loss:0.2128150536741247\n",
      "train loss:0.26919893135427886\n",
      "train loss:0.2110819427453971\n",
      "train loss:0.3769715672292987\n",
      "train loss:0.2832200197293484\n",
      "train loss:0.19762571854509428\n",
      "train loss:0.24215558481497218\n",
      "train loss:0.1907360292268803\n",
      "train loss:0.2348286000150339\n",
      "train loss:0.30978430243709576\n",
      "train loss:0.14749987461867206\n",
      "train loss:0.2309831718318719\n",
      "train loss:0.3257451244309817\n",
      "train loss:0.1898924267971495\n",
      "train loss:0.22362206874115262\n",
      "train loss:0.2552650997543678\n",
      "train loss:0.36243944458013017\n",
      "train loss:0.32823754007896366\n",
      "train loss:0.2360466109167669\n",
      "train loss:0.21872763309613344\n",
      "train loss:0.3673143924504231\n",
      "train loss:0.22962534563659115\n",
      "train loss:0.19769569063111658\n",
      "train loss:0.3353933681433135\n",
      "train loss:0.33107486987956064\n",
      "train loss:0.2788177514346132\n",
      "train loss:0.28471457385790405\n",
      "train loss:0.17857537857516617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1852560559065738\n",
      "train loss:0.2062120780353868\n",
      "train loss:0.18315281011863085\n",
      "train loss:0.267439825884637\n",
      "train loss:0.3320055004440776\n",
      "train loss:0.254927605715719\n",
      "train loss:0.18830028609523153\n",
      "train loss:0.2907441147971175\n",
      "train loss:0.1528988786847688\n",
      "train loss:0.16175426596271603\n",
      "train loss:0.14295280249013784\n",
      "train loss:0.1875808121602347\n",
      "train loss:0.14053023758297706\n",
      "train loss:0.16776037845539238\n",
      "train loss:0.14344512975524043\n",
      "train loss:0.1937750717411771\n",
      "train loss:0.2221613613591888\n",
      "train loss:0.2592061439983357\n",
      "train loss:0.22910218476904093\n",
      "train loss:0.17787676335696276\n",
      "train loss:0.23414442939982671\n",
      "train loss:0.20668013206944025\n",
      "train loss:0.16599077418407607\n",
      "train loss:0.24617379219861696\n",
      "train loss:0.22483720492689777\n",
      "train loss:0.3113948139169825\n",
      "train loss:0.20585500964847056\n",
      "train loss:0.24731642549730257\n",
      "train loss:0.24231794466472567\n",
      "train loss:0.1339605266137326\n",
      "train loss:0.21114914834890874\n",
      "train loss:0.1703926980438763\n",
      "train loss:0.16840253929503826\n",
      "train loss:0.20044779596072915\n",
      "train loss:0.12576470955446734\n",
      "train loss:0.1902147940014171\n",
      "train loss:0.10818480786248542\n",
      "train loss:0.19200784245318608\n",
      "train loss:0.1543358758155417\n",
      "train loss:0.3039060823274613\n",
      "train loss:0.3169511184228295\n",
      "train loss:0.2407635374351171\n",
      "train loss:0.2833941834168378\n",
      "train loss:0.2277112786718676\n",
      "train loss:0.24041446451840065\n",
      "train loss:0.26564776749411695\n",
      "train loss:0.24121305485584685\n",
      "train loss:0.19063198387418315\n",
      "train loss:0.2805779046636349\n",
      "train loss:0.26718112591254917\n",
      "train loss:0.21989587480327402\n",
      "train loss:0.15494176557602785\n",
      "train loss:0.18220725963870998\n",
      "train loss:0.17004849473038774\n",
      "train loss:0.2902672388746417\n",
      "train loss:0.3602710217706573\n",
      "train loss:0.23172111309494892\n",
      "train loss:0.26327791146215607\n",
      "train loss:0.23800724136373716\n",
      "train loss:0.17947939978925928\n",
      "train loss:0.25877051313564514\n",
      "train loss:0.23167635002962997\n",
      "train loss:0.33521398915455203\n",
      "train loss:0.2073384191528278\n",
      "train loss:0.10668828193295163\n",
      "train loss:0.2296511995822107\n",
      "train loss:0.21168741210797823\n",
      "train loss:0.2387269051374675\n",
      "train loss:0.2738656875758892\n",
      "train loss:0.11502870935854718\n",
      "train loss:0.22217480937664846\n",
      "train loss:0.17484896685647036\n",
      "train loss:0.19350779479666158\n",
      "train loss:0.10676005418156147\n",
      "train loss:0.1620202592341273\n",
      "train loss:0.337821508017247\n",
      "train loss:0.11067616041603942\n",
      "train loss:0.22993135789279062\n",
      "train loss:0.21258040057409047\n",
      "train loss:0.18283302476706417\n",
      "train loss:0.1354927151764394\n",
      "train loss:0.20202294028538106\n",
      "train loss:0.42032141199004786\n",
      "train loss:0.09742031763561514\n",
      "train loss:0.12133777779464715\n",
      "train loss:0.18427295381628667\n",
      "train loss:0.27605783592327504\n",
      "train loss:0.2546687174673039\n",
      "train loss:0.23331001062070073\n",
      "train loss:0.15517387877497324\n",
      "train loss:0.28700199838052753\n",
      "train loss:0.2952852874394674\n",
      "train loss:0.13381467910196537\n",
      "train loss:0.2062362706786927\n",
      "train loss:0.20833479733018453\n",
      "train loss:0.16871966221924758\n",
      "train loss:0.18847212302832278\n",
      "train loss:0.10246053372615574\n",
      "train loss:0.09786447587820689\n",
      "train loss:0.1720132275788534\n",
      "train loss:0.22190996002976363\n",
      "train loss:0.15419025263674008\n",
      "train loss:0.1807179757087244\n",
      "train loss:0.21787560717915924\n",
      "train loss:0.17176367237085116\n",
      "train loss:0.13807524932217674\n",
      "train loss:0.19323541679849246\n",
      "train loss:0.13311173472973187\n",
      "train loss:0.15990912520572378\n",
      "train loss:0.12042629732873927\n",
      "train loss:0.15584188078346076\n",
      "train loss:0.20776862532542462\n",
      "train loss:0.20192943113472528\n",
      "train loss:0.1186826654299548\n",
      "train loss:0.13650003286647938\n",
      "train loss:0.2526733010285861\n",
      "train loss:0.1175949587140849\n",
      "train loss:0.19523047224983361\n",
      "train loss:0.1395387651555443\n",
      "train loss:0.23647470826618133\n",
      "train loss:0.22352385913715875\n",
      "train loss:0.18188601475310415\n",
      "train loss:0.14764918390880574\n",
      "train loss:0.11529932150051973\n",
      "train loss:0.17559220976139822\n",
      "train loss:0.24902807142231062\n",
      "train loss:0.2262899116546705\n",
      "train loss:0.10656794232421278\n",
      "train loss:0.3195387754283232\n",
      "train loss:0.12013771096267421\n",
      "train loss:0.16306035645303166\n",
      "train loss:0.14037522043129036\n",
      "train loss:0.2487322521769885\n",
      "train loss:0.1546080231685121\n",
      "train loss:0.1803709778517892\n",
      "train loss:0.14533452699188487\n",
      "train loss:0.14847142084026765\n",
      "train loss:0.21976926680197184\n",
      "train loss:0.09627313337674576\n",
      "train loss:0.1557758480394917\n",
      "train loss:0.1435361121515988\n",
      "train loss:0.12680355379008929\n",
      "train loss:0.13603247588798784\n",
      "train loss:0.1503661676106599\n",
      "train loss:0.21597400112838908\n",
      "train loss:0.16814779774684457\n",
      "train loss:0.11167036075588593\n",
      "train loss:0.26480450722905546\n",
      "train loss:0.16230385343942577\n",
      "train loss:0.16169186791808854\n",
      "train loss:0.2611257685314399\n",
      "train loss:0.2290468279838026\n",
      "train loss:0.07121248700681604\n",
      "train loss:0.23413885582323649\n",
      "train loss:0.14661658066812494\n",
      "train loss:0.2021772257107429\n",
      "train loss:0.13998873922951202\n",
      "train loss:0.1456455200078678\n",
      "train loss:0.13844872838811034\n",
      "train loss:0.10214393831422357\n",
      "train loss:0.09233375676815489\n",
      "train loss:0.13205792881165918\n",
      "train loss:0.2523628662768444\n",
      "train loss:0.09664751247293926\n",
      "train loss:0.15919546699986464\n",
      "train loss:0.10864792897722587\n",
      "train loss:0.11445378542209989\n",
      "train loss:0.11878150757165672\n",
      "train loss:0.09489645616305423\n",
      "train loss:0.12934256907032501\n",
      "train loss:0.11419293671220421\n",
      "train loss:0.1857094764138973\n",
      "train loss:0.10273135449430366\n",
      "train loss:0.23834599370973975\n",
      "train loss:0.09840219281567032\n",
      "train loss:0.07385105477954007\n",
      "train loss:0.27695794740950685\n",
      "train loss:0.1334199168355017\n",
      "train loss:0.11870096763730066\n",
      "train loss:0.16747072227925203\n",
      "train loss:0.2142551109619844\n",
      "train loss:0.08184011869986531\n",
      "train loss:0.14329790017856867\n",
      "train loss:0.18061103701176923\n",
      "train loss:0.08420973036220764\n",
      "train loss:0.22130048653656112\n",
      "train loss:0.21111940490071235\n",
      "train loss:0.0783600965499262\n",
      "train loss:0.2041292987424168\n",
      "train loss:0.10904980762799057\n",
      "train loss:0.14378623360446163\n",
      "train loss:0.1294923336432557\n",
      "train loss:0.18218604217976656\n",
      "train loss:0.06711126805476891\n",
      "train loss:0.19092229629337193\n",
      "train loss:0.1208033790220882\n",
      "train loss:0.09138313924379514\n",
      "train loss:0.19683675057212568\n",
      "train loss:0.09071213240004732\n",
      "train loss:0.06378946073102627\n",
      "train loss:0.09466349397108248\n",
      "train loss:0.0906218097150084\n",
      "train loss:0.07697800978166339\n",
      "train loss:0.1445013756975382\n",
      "train loss:0.10183503218443583\n",
      "train loss:0.125948570063948\n",
      "train loss:0.14701031929228262\n",
      "train loss:0.12914459522273225\n",
      "train loss:0.14005982629346025\n",
      "train loss:0.13297403699106952\n",
      "train loss:0.09340051214759226\n",
      "train loss:0.11763657283460736\n",
      "train loss:0.07889764349707015\n",
      "train loss:0.26291288843301586\n",
      "train loss:0.1312504070517694\n",
      "train loss:0.1319928966861072\n",
      "train loss:0.171184006309531\n",
      "train loss:0.10927591396170434\n",
      "train loss:0.09564105572891479\n",
      "train loss:0.1806544642151906\n",
      "train loss:0.13374972134741386\n",
      "train loss:0.14510779648138886\n",
      "train loss:0.18790266797722283\n",
      "train loss:0.2503454780789315\n",
      "train loss:0.17009941065954276\n",
      "train loss:0.14546992424198682\n",
      "train loss:0.16434025924258777\n",
      "train loss:0.1454504294844995\n",
      "train loss:0.12456502953591274\n",
      "train loss:0.1391460301452843\n",
      "train loss:0.13013695744637716\n",
      "train loss:0.15956302611181045\n",
      "train loss:0.1261687816172138\n",
      "train loss:0.2688557650162375\n",
      "train loss:0.14575302561395206\n",
      "train loss:0.09198731033161822\n",
      "train loss:0.28385413039999235\n",
      "train loss:0.18742836766419294\n",
      "train loss:0.14460801391702066\n",
      "train loss:0.17939363453067997\n",
      "train loss:0.11195173574403407\n",
      "train loss:0.14244056900034094\n",
      "train loss:0.08832628996042055\n",
      "train loss:0.21839128012474124\n",
      "train loss:0.10727684186044623\n",
      "train loss:0.20339098834861932\n",
      "train loss:0.23396336044033172\n",
      "train loss:0.33868224478492187\n",
      "train loss:0.17726883045948647\n",
      "train loss:0.1272013710584137\n",
      "train loss:0.24125651379341775\n",
      "train loss:0.15678829738908148\n",
      "train loss:0.10181370082887799\n",
      "train loss:0.2968863147472691\n",
      "train loss:0.1973652604242449\n",
      "train loss:0.16292099795401796\n",
      "train loss:0.07960827196155926\n",
      "train loss:0.19297918054518465\n",
      "train loss:0.22191560269420285\n",
      "train loss:0.11933798244412008\n",
      "train loss:0.1390600346215895\n",
      "train loss:0.13027476233756952\n",
      "train loss:0.15883864838342504\n",
      "train loss:0.08489456960270435\n",
      "train loss:0.07973220227397712\n",
      "train loss:0.10916183564209148\n",
      "train loss:0.17164134609129011\n",
      "train loss:0.1902197723841785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1037363878203857\n",
      "train loss:0.0891398717327097\n",
      "train loss:0.08915147451808927\n",
      "train loss:0.15374463688012815\n",
      "train loss:0.16568665946674485\n",
      "train loss:0.09396703655393404\n",
      "train loss:0.059156130451130655\n",
      "train loss:0.13291499995955125\n",
      "train loss:0.12279126007706591\n",
      "train loss:0.05859938763724993\n",
      "train loss:0.14339631595506386\n",
      "train loss:0.1587651088288751\n",
      "train loss:0.10682659853158849\n",
      "train loss:0.06754509203084381\n",
      "train loss:0.07609023105268775\n",
      "train loss:0.1979022198609616\n",
      "train loss:0.07625675474506606\n",
      "train loss:0.11268001978534169\n",
      "train loss:0.08321896030971965\n",
      "train loss:0.06506257732739211\n",
      "train loss:0.07343680482096393\n",
      "train loss:0.13330858438434437\n",
      "train loss:0.21653473420124314\n",
      "train loss:0.05912253521869504\n",
      "train loss:0.092149691167905\n",
      "train loss:0.08718057001312791\n",
      "train loss:0.20008736822641596\n",
      "train loss:0.09553366519272691\n",
      "train loss:0.07099306542232832\n",
      "train loss:0.08065915043915255\n",
      "train loss:0.1701298134261882\n",
      "train loss:0.21467289867157166\n",
      "train loss:0.08931911051309706\n",
      "train loss:0.18164099660070485\n",
      "train loss:0.0768957641618928\n",
      "train loss:0.06884016376495333\n",
      "train loss:0.10798612035933022\n",
      "train loss:0.1236178836390906\n",
      "train loss:0.13623668246753212\n",
      "train loss:0.10498121334433622\n",
      "train loss:0.0840313395071916\n",
      "train loss:0.10366273021088\n",
      "train loss:0.21854574033039495\n",
      "train loss:0.10785545430870869\n",
      "train loss:0.09319199549978283\n",
      "train loss:0.15258044653746505\n",
      "train loss:0.06264734387310882\n",
      "train loss:0.08607368339422702\n",
      "train loss:0.23414342075967554\n",
      "train loss:0.09311395936766409\n",
      "train loss:0.15984237244532437\n",
      "train loss:0.10878222909348806\n",
      "train loss:0.26777071930306756\n",
      "train loss:0.27021407104346884\n",
      "train loss:0.05740293298913564\n",
      "train loss:0.05051535748337332\n",
      "train loss:0.15361526665588077\n",
      "train loss:0.23721432513947602\n",
      "train loss:0.11464049579548105\n",
      "train loss:0.11028501332880537\n",
      "train loss:0.18048482554148207\n",
      "train loss:0.06484129295418219\n",
      "train loss:0.10183647099763964\n",
      "=== epoch:2, train acc:0.963, test acc:0.967 ===\n",
      "train loss:0.14630512598702206\n",
      "train loss:0.14026216365918542\n",
      "train loss:0.18459030309836233\n",
      "train loss:0.04523921816111068\n",
      "train loss:0.11944972972959184\n",
      "train loss:0.09375426678047992\n",
      "train loss:0.1455471979187765\n",
      "train loss:0.1322531205601043\n",
      "train loss:0.14087959060065344\n",
      "train loss:0.15800752366297305\n",
      "train loss:0.15636731155762992\n",
      "train loss:0.14657293119552003\n",
      "train loss:0.1298169334554427\n",
      "train loss:0.23223657677845963\n",
      "train loss:0.13339134272810743\n",
      "train loss:0.1509881948665676\n",
      "train loss:0.08752877370849017\n",
      "train loss:0.2224608641351896\n",
      "train loss:0.1041909617999924\n",
      "train loss:0.10178522285955995\n",
      "train loss:0.1894540582886639\n",
      "train loss:0.15738011077296168\n",
      "train loss:0.10232569733371105\n",
      "train loss:0.07750391059122422\n",
      "train loss:0.08817029415107877\n",
      "train loss:0.07388686537432904\n",
      "train loss:0.04133638010204713\n",
      "train loss:0.11648111615795344\n",
      "train loss:0.1769890636759114\n",
      "train loss:0.1432596098460586\n",
      "train loss:0.05639065679393065\n",
      "train loss:0.14358978648132079\n",
      "train loss:0.11582045328683746\n",
      "train loss:0.078526387506359\n",
      "train loss:0.08348442514551502\n",
      "train loss:0.12622413538825128\n",
      "train loss:0.10460764864703855\n",
      "train loss:0.07768745117528114\n",
      "train loss:0.08135482511908453\n",
      "train loss:0.10467098007055388\n",
      "train loss:0.08801035389225985\n",
      "train loss:0.21608642604931202\n",
      "train loss:0.12376329704547187\n",
      "train loss:0.14051606680296225\n",
      "train loss:0.13697883238644668\n",
      "train loss:0.11527900401884211\n",
      "train loss:0.08021284239245681\n",
      "train loss:0.17901253680062065\n",
      "train loss:0.15005616284374043\n",
      "train loss:0.09844586921514369\n",
      "train loss:0.15733048203195815\n",
      "train loss:0.03489627880100655\n",
      "train loss:0.07729270031351382\n",
      "train loss:0.09856599537656882\n",
      "train loss:0.08079862580879676\n",
      "train loss:0.08486952469802449\n",
      "train loss:0.10463934578946096\n",
      "train loss:0.16292362452701126\n",
      "train loss:0.06727354510585819\n",
      "train loss:0.1356345984454971\n",
      "train loss:0.17813587862151462\n",
      "train loss:0.12630227827920856\n",
      "train loss:0.10105870975905855\n",
      "train loss:0.06254384309526803\n",
      "train loss:0.15453845705744276\n",
      "train loss:0.16176440564769823\n",
      "train loss:0.09955779804769514\n",
      "train loss:0.06323373379792155\n",
      "train loss:0.05086930888986454\n",
      "train loss:0.08825207031736673\n",
      "train loss:0.11531944584409243\n",
      "train loss:0.10488252033624366\n",
      "train loss:0.13004408774585352\n",
      "train loss:0.26371538549294554\n",
      "train loss:0.0699483518672468\n",
      "train loss:0.08620648326820986\n",
      "train loss:0.04657847184866656\n",
      "train loss:0.12919527289785215\n",
      "train loss:0.09716274281377241\n",
      "train loss:0.13793536218836505\n",
      "train loss:0.1072397519294852\n",
      "train loss:0.10047794281368788\n",
      "train loss:0.12371516641744006\n",
      "train loss:0.07211302919708337\n",
      "train loss:0.11437653772692576\n",
      "train loss:0.11968127935770781\n",
      "train loss:0.15645264845151038\n",
      "train loss:0.06341716716015004\n",
      "train loss:0.17634909691936143\n",
      "train loss:0.1354965089438825\n",
      "train loss:0.11990079594114614\n",
      "train loss:0.07014870141696025\n",
      "train loss:0.10031356178660374\n",
      "train loss:0.09509268311215507\n",
      "train loss:0.07929228510618232\n",
      "train loss:0.11173800260204753\n",
      "train loss:0.08470275297886946\n",
      "train loss:0.1437720084307133\n",
      "train loss:0.15229101186512187\n",
      "train loss:0.1559649380504094\n",
      "train loss:0.10908074997497394\n",
      "train loss:0.11335028201195328\n",
      "train loss:0.10716421414254688\n",
      "train loss:0.10326276653701844\n",
      "train loss:0.04039062531883046\n",
      "train loss:0.10670996704415932\n",
      "train loss:0.08170207189432517\n",
      "train loss:0.07136935557727914\n",
      "train loss:0.14244009346854625\n",
      "train loss:0.11157724948838649\n",
      "train loss:0.09977987892434756\n",
      "train loss:0.14555624433130726\n",
      "train loss:0.09388374635420375\n",
      "train loss:0.12786691470050335\n",
      "train loss:0.053456308322647444\n",
      "train loss:0.07062406990550786\n",
      "train loss:0.11696710001025735\n",
      "train loss:0.27088417476376553\n",
      "train loss:0.11245392999750849\n",
      "train loss:0.0424357266010635\n",
      "train loss:0.1016184115490793\n",
      "train loss:0.06724505183041925\n",
      "train loss:0.08861586003374354\n",
      "train loss:0.11371945916567615\n",
      "train loss:0.1097150979565838\n",
      "train loss:0.14660224549829473\n",
      "train loss:0.11803378416484474\n",
      "train loss:0.04802932139201406\n",
      "train loss:0.07576360830101185\n",
      "train loss:0.10712910448118858\n",
      "train loss:0.07036329636541289\n",
      "train loss:0.06275643793645443\n",
      "train loss:0.04291119849029402\n",
      "train loss:0.10876553272171315\n",
      "train loss:0.07092895926972317\n",
      "train loss:0.06426577464596891\n",
      "train loss:0.07041640950812247\n",
      "train loss:0.1439985168684088\n",
      "train loss:0.14092136029393829\n",
      "train loss:0.07699655469919464\n",
      "train loss:0.13892736275734485\n",
      "train loss:0.12884117052213157\n",
      "train loss:0.07044601477256394\n",
      "train loss:0.07976335550701695\n",
      "train loss:0.06737670631464875\n",
      "train loss:0.08917559979887126\n",
      "train loss:0.055535208849259006\n",
      "train loss:0.11761020967856275\n",
      "train loss:0.06127548021208365\n",
      "train loss:0.09970822677408092\n",
      "train loss:0.14003295669672308\n",
      "train loss:0.06032824263406445\n",
      "train loss:0.09685249007400738\n",
      "train loss:0.1315294388177921\n",
      "train loss:0.05966226684604997\n",
      "train loss:0.2262227383041268\n",
      "train loss:0.13815517793070847\n",
      "train loss:0.06121221937123414\n",
      "train loss:0.0883944586872506\n",
      "train loss:0.13010395166740119\n",
      "train loss:0.198104717230647\n",
      "train loss:0.06974152294966891\n",
      "train loss:0.05916489821987961\n",
      "train loss:0.03882509640048119\n",
      "train loss:0.07486372954528285\n",
      "train loss:0.10103691399866337\n",
      "train loss:0.13137053234139814\n",
      "train loss:0.051515717361901185\n",
      "train loss:0.15661528528653748\n",
      "train loss:0.10815685974837703\n",
      "train loss:0.07990401634484832\n",
      "train loss:0.15641720693655528\n",
      "train loss:0.09099507328349431\n",
      "train loss:0.06518078739945637\n",
      "train loss:0.14115921608005413\n",
      "train loss:0.08211333449656238\n",
      "train loss:0.1604028634084303\n",
      "train loss:0.07706422254632286\n",
      "train loss:0.08115252890797703\n",
      "train loss:0.07152259112858811\n",
      "train loss:0.125657320925551\n",
      "train loss:0.07419669723997935\n",
      "train loss:0.06451280439141414\n",
      "train loss:0.06878269007042548\n",
      "train loss:0.15259079268003856\n",
      "train loss:0.05741064982644378\n",
      "train loss:0.09445571389899729\n",
      "train loss:0.14174743668200696\n",
      "train loss:0.1765165189038437\n",
      "train loss:0.10626091612234508\n",
      "train loss:0.0595703037527297\n",
      "train loss:0.206411045074613\n",
      "train loss:0.07308976520347892\n",
      "train loss:0.036209175549892454\n",
      "train loss:0.07371747637109768\n",
      "train loss:0.2017862879872211\n",
      "train loss:0.04683812797290852\n",
      "train loss:0.04091343705305439\n",
      "train loss:0.05766996519509437\n",
      "train loss:0.1000244001173953\n",
      "train loss:0.13589747800270927\n",
      "train loss:0.05401391032941735\n",
      "train loss:0.1240581509026815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03431307099325209\n",
      "train loss:0.08384914073735877\n",
      "train loss:0.1627583157667351\n",
      "train loss:0.040232736390679945\n",
      "train loss:0.06967233391887012\n",
      "train loss:0.09885236721704196\n",
      "train loss:0.11481590994892071\n",
      "train loss:0.09364225016878472\n",
      "train loss:0.11467969128195772\n",
      "train loss:0.08402582276851506\n",
      "train loss:0.030312675508224647\n",
      "train loss:0.08123704064270543\n",
      "train loss:0.033841278523350754\n",
      "train loss:0.17318770365591887\n",
      "train loss:0.05576301772634729\n",
      "train loss:0.042262805222973494\n",
      "train loss:0.11754815461745398\n",
      "train loss:0.044366719606962104\n",
      "train loss:0.13787378470431463\n",
      "train loss:0.05276390008525953\n",
      "train loss:0.11318445224845444\n",
      "train loss:0.1671674477210546\n",
      "train loss:0.02561681844574399\n",
      "train loss:0.05895113963160209\n",
      "train loss:0.08809755933405906\n",
      "train loss:0.07541005526281713\n",
      "train loss:0.03376032382735023\n",
      "train loss:0.07527305858771989\n",
      "train loss:0.04820216353916572\n",
      "train loss:0.11635266020961792\n",
      "train loss:0.12595767249593062\n",
      "train loss:0.1672130750959089\n",
      "train loss:0.09547262332155261\n",
      "train loss:0.15735994041770884\n",
      "train loss:0.08992521222308177\n",
      "train loss:0.14578243908819272\n",
      "train loss:0.055526474451404406\n",
      "train loss:0.1183035448870735\n",
      "train loss:0.0518669654122629\n",
      "train loss:0.10860793953577591\n",
      "train loss:0.08835867881473022\n",
      "train loss:0.11708869784374533\n",
      "train loss:0.029293242641947166\n",
      "train loss:0.0639193937752899\n",
      "train loss:0.06455219882920861\n",
      "train loss:0.03582119872510459\n",
      "train loss:0.11830052892286048\n",
      "train loss:0.03334232729341087\n",
      "train loss:0.043353121940378464\n",
      "train loss:0.15456669326638853\n",
      "train loss:0.09600289495086028\n",
      "train loss:0.05006760000191426\n",
      "train loss:0.03224778451659959\n",
      "train loss:0.0873634295517459\n",
      "train loss:0.06823837866197162\n",
      "train loss:0.0946845915011568\n",
      "train loss:0.07287286383531938\n",
      "train loss:0.034711593581527936\n",
      "train loss:0.12430024883725432\n",
      "train loss:0.0579117181428671\n",
      "train loss:0.1048944755456087\n",
      "train loss:0.04481172856313362\n",
      "train loss:0.03302739227710241\n",
      "train loss:0.06254469707641384\n",
      "train loss:0.11412920367486544\n",
      "train loss:0.07416182249629898\n",
      "train loss:0.043156982688153066\n",
      "train loss:0.20129995373790888\n",
      "train loss:0.10596991285888155\n",
      "train loss:0.12539390431626313\n",
      "train loss:0.0367372417906843\n",
      "train loss:0.02845316333345669\n",
      "train loss:0.10209405521574519\n",
      "train loss:0.05194233820261372\n",
      "train loss:0.06374462980661112\n",
      "train loss:0.07926670528206572\n",
      "train loss:0.13266709556737108\n",
      "train loss:0.05344064366509241\n",
      "train loss:0.06839501525408545\n",
      "train loss:0.0665766328324559\n",
      "train loss:0.06614556474546784\n",
      "train loss:0.08827922361566926\n",
      "train loss:0.09404365270398862\n",
      "train loss:0.061040969953578925\n",
      "train loss:0.07332417909981279\n",
      "train loss:0.07513635116933905\n",
      "train loss:0.09175471757029463\n",
      "train loss:0.11709271329849281\n",
      "train loss:0.0712470339080495\n",
      "train loss:0.11318146668019662\n",
      "train loss:0.12954378999637972\n",
      "train loss:0.12087556098652254\n",
      "train loss:0.05325941479513514\n",
      "train loss:0.14186221895956794\n",
      "train loss:0.10319809472368374\n",
      "train loss:0.07913134999315785\n",
      "train loss:0.033059308816510265\n",
      "train loss:0.10595318611275709\n",
      "train loss:0.07094693084348153\n",
      "train loss:0.15545739742025788\n",
      "train loss:0.09272235387512977\n",
      "train loss:0.051122525714643235\n",
      "train loss:0.09769258281068442\n",
      "train loss:0.12365887111869657\n",
      "train loss:0.06881333345719756\n",
      "train loss:0.1105268186701986\n",
      "train loss:0.01875328751975853\n",
      "train loss:0.06980742277120737\n",
      "train loss:0.035797035233091655\n",
      "train loss:0.03852649120345891\n",
      "train loss:0.15747142488252885\n",
      "train loss:0.05847875179916176\n",
      "train loss:0.03356166130199635\n",
      "train loss:0.06632338985843317\n",
      "train loss:0.06986036313762872\n",
      "train loss:0.09050063558808379\n",
      "train loss:0.12998437569818344\n",
      "train loss:0.0766589008796566\n",
      "train loss:0.19554445920932242\n",
      "train loss:0.03407167237910123\n",
      "train loss:0.11017026149412784\n",
      "train loss:0.03941446943858187\n",
      "train loss:0.043582378214860454\n",
      "train loss:0.1619453434495399\n",
      "train loss:0.05645702659317621\n",
      "train loss:0.018599158121909175\n",
      "train loss:0.09379799949034838\n",
      "train loss:0.14272363550059933\n",
      "train loss:0.06428002801108078\n",
      "train loss:0.1657098155969698\n",
      "train loss:0.06499924191697114\n",
      "train loss:0.07637071048497743\n",
      "train loss:0.09178775999750349\n",
      "train loss:0.07164872050415715\n",
      "train loss:0.16247647409854493\n",
      "train loss:0.07851687776041665\n",
      "train loss:0.0736614556716153\n",
      "train loss:0.08732832012426378\n",
      "train loss:0.14931930280919178\n",
      "train loss:0.03271033796672439\n",
      "train loss:0.06898203617384827\n",
      "train loss:0.051620381093168355\n",
      "train loss:0.07722661538880496\n",
      "train loss:0.05078657893823886\n",
      "train loss:0.10128777852545494\n",
      "train loss:0.1293824069888853\n",
      "train loss:0.05685339673035213\n",
      "train loss:0.13746890487088714\n",
      "train loss:0.11765065740977165\n",
      "train loss:0.07196691303325524\n",
      "train loss:0.1015289322590613\n",
      "train loss:0.09754822752210521\n",
      "train loss:0.10856672833216624\n",
      "train loss:0.08144124673963336\n",
      "train loss:0.10368412126220784\n",
      "train loss:0.08223484970768068\n",
      "train loss:0.034900120922169736\n",
      "train loss:0.09246151094416026\n",
      "train loss:0.11681404120981295\n",
      "train loss:0.10060456464495476\n",
      "train loss:0.134645403042521\n",
      "train loss:0.02703537429067517\n",
      "train loss:0.09206014785091429\n",
      "train loss:0.13000618690660684\n",
      "train loss:0.05910970165436579\n",
      "train loss:0.08006385048380098\n",
      "train loss:0.06258570180535501\n",
      "train loss:0.14439922433022462\n",
      "train loss:0.053181657681992595\n",
      "train loss:0.1354201642680174\n",
      "train loss:0.04085761450439723\n",
      "train loss:0.1591479880880627\n",
      "train loss:0.08884487113574696\n",
      "train loss:0.1569012036680641\n",
      "train loss:0.02167975003892373\n",
      "train loss:0.08432348508096321\n",
      "train loss:0.0330529481224516\n",
      "train loss:0.012204019774312864\n",
      "train loss:0.0805955572250286\n",
      "train loss:0.1926747911710861\n",
      "train loss:0.10139860841264017\n",
      "train loss:0.10769956081888553\n",
      "train loss:0.05722236114619908\n",
      "train loss:0.053098573734120046\n",
      "train loss:0.04189515443337943\n",
      "train loss:0.06110849903427242\n",
      "train loss:0.145684691374402\n",
      "train loss:0.038988128784609305\n",
      "train loss:0.0623662715544772\n",
      "train loss:0.01297583781047046\n",
      "train loss:0.07882644411719618\n",
      "train loss:0.15781232507192244\n",
      "train loss:0.06885159161282611\n",
      "train loss:0.029919243691985186\n",
      "train loss:0.04238336005702564\n",
      "train loss:0.09896940819286457\n",
      "train loss:0.06434737846978643\n",
      "train loss:0.04618354521004084\n",
      "train loss:0.04933940236271275\n",
      "train loss:0.04605362274991578\n",
      "train loss:0.03563442069630399\n",
      "train loss:0.015115867576261523\n",
      "train loss:0.058432175774795904\n",
      "train loss:0.0685311303692417\n",
      "train loss:0.08314200222744628\n",
      "train loss:0.04421999380963332\n",
      "train loss:0.02129496504701086\n",
      "train loss:0.08972550779038059\n",
      "train loss:0.05285028926834572\n",
      "train loss:0.028601165600936538\n",
      "train loss:0.19801870552092152\n",
      "train loss:0.021693141355431746\n",
      "train loss:0.03151712482490197\n",
      "train loss:0.055787946399485346\n",
      "train loss:0.03647114709880562\n",
      "train loss:0.1269140480015449\n",
      "train loss:0.040549500992246496\n",
      "train loss:0.08975666287714416\n",
      "train loss:0.0965006314546704\n",
      "train loss:0.02896251062686027\n",
      "train loss:0.03676718199587572\n",
      "train loss:0.07274769803389844\n",
      "train loss:0.0844629798083749\n",
      "train loss:0.16457380755552953\n",
      "train loss:0.07113843565496727\n",
      "train loss:0.1984247069103949\n",
      "train loss:0.1042234389352435\n",
      "train loss:0.09611215145992373\n",
      "train loss:0.06416975835458047\n",
      "train loss:0.03365539208464721\n",
      "train loss:0.12322910648289355\n",
      "train loss:0.024541326212694704\n",
      "train loss:0.10413446592932459\n",
      "train loss:0.06286356255275014\n",
      "train loss:0.028754124632988973\n",
      "train loss:0.021017817175479698\n",
      "train loss:0.023739746699394156\n",
      "train loss:0.08295899716033167\n",
      "train loss:0.05542313357692554\n",
      "train loss:0.16901107864338247\n",
      "train loss:0.02154402009428945\n",
      "train loss:0.041944184153380484\n",
      "train loss:0.0418978475586195\n",
      "train loss:0.07420661319210058\n",
      "train loss:0.055273472310620984\n",
      "train loss:0.0595486150924678\n",
      "train loss:0.17465654348092868\n",
      "train loss:0.028282664154650205\n",
      "train loss:0.03824647021224648\n",
      "train loss:0.06348997857492505\n",
      "train loss:0.07639335825048892\n",
      "train loss:0.017602993515577453\n",
      "train loss:0.04878193353181746\n",
      "train loss:0.056162108519595855\n",
      "train loss:0.05253671838948273\n",
      "train loss:0.03198315785901423\n",
      "train loss:0.25604128379037583\n",
      "train loss:0.06518544340251449\n",
      "train loss:0.036547758662075644\n",
      "train loss:0.1966269663113568\n",
      "train loss:0.03761805211463332\n",
      "train loss:0.026898917022198408\n",
      "train loss:0.052446964674484245\n",
      "train loss:0.1644360100034763\n",
      "train loss:0.06794898123284482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06528832976470549\n",
      "train loss:0.06096889307921291\n",
      "train loss:0.10075710807806977\n",
      "train loss:0.08144042704022235\n",
      "train loss:0.049844252765416416\n",
      "train loss:0.0825830233381219\n",
      "train loss:0.03672397846030925\n",
      "train loss:0.10324618639709385\n",
      "train loss:0.09448093013763076\n",
      "train loss:0.04349308859413623\n",
      "train loss:0.1487353068559347\n",
      "train loss:0.17040150895299283\n",
      "train loss:0.05213624767520773\n",
      "train loss:0.031508975803240725\n",
      "train loss:0.055593835033109676\n",
      "train loss:0.04759335946415332\n",
      "train loss:0.04644287576293778\n",
      "train loss:0.032135233515298595\n",
      "train loss:0.07751893558436619\n",
      "train loss:0.056872830861699544\n",
      "train loss:0.03724488766349428\n",
      "train loss:0.15198774796929768\n",
      "train loss:0.04057342689315273\n",
      "train loss:0.08105333229704381\n",
      "train loss:0.03418901976176445\n",
      "train loss:0.08010867388904627\n",
      "train loss:0.07439392355178308\n",
      "train loss:0.12767108167051502\n",
      "train loss:0.06118208998129265\n",
      "train loss:0.06621808151047284\n",
      "train loss:0.08285067829974155\n",
      "train loss:0.08804380641798232\n",
      "train loss:0.08333063002881513\n",
      "train loss:0.055642711730045986\n",
      "train loss:0.07365779682582442\n",
      "train loss:0.03973253833378726\n",
      "train loss:0.008991955005339627\n",
      "train loss:0.0491072206169029\n",
      "train loss:0.03690171730057609\n",
      "train loss:0.10127171493876314\n",
      "train loss:0.0645992674670587\n",
      "train loss:0.14375700517005735\n",
      "train loss:0.10164568910499507\n",
      "train loss:0.042861935462367516\n",
      "train loss:0.03753651973339353\n",
      "train loss:0.12709149379918144\n",
      "train loss:0.11294309037687231\n",
      "train loss:0.07463152215909047\n",
      "train loss:0.06998037226301915\n",
      "train loss:0.05056807486633014\n",
      "train loss:0.13320293367842104\n",
      "train loss:0.04132628604360071\n",
      "train loss:0.04118058357345969\n",
      "train loss:0.0512586590539339\n",
      "train loss:0.1526297141713332\n",
      "train loss:0.11000895240614318\n",
      "train loss:0.03241090704263117\n",
      "train loss:0.11445304779782178\n",
      "train loss:0.09707818424544574\n",
      "train loss:0.05032115041870126\n",
      "train loss:0.03985501914214957\n",
      "train loss:0.04187277317481728\n",
      "train loss:0.0945567399564591\n",
      "train loss:0.09175932982133515\n",
      "train loss:0.07099375715938945\n",
      "train loss:0.09148238270337669\n",
      "train loss:0.1284711090686596\n",
      "train loss:0.06138723827889292\n",
      "train loss:0.04842932644915587\n",
      "train loss:0.02820537407745032\n",
      "train loss:0.06973805965918968\n",
      "train loss:0.09557690073822402\n",
      "train loss:0.06871690202566254\n",
      "train loss:0.06616104806423785\n",
      "train loss:0.0863660067350857\n",
      "train loss:0.10867344255482793\n",
      "train loss:0.043286361255626356\n",
      "train loss:0.08124348809404766\n",
      "train loss:0.04317522884652985\n",
      "train loss:0.040530290882502484\n",
      "train loss:0.019364227492427288\n",
      "train loss:0.025829844060102344\n",
      "train loss:0.017493870158633767\n",
      "train loss:0.0679748282558984\n",
      "train loss:0.09863725318537674\n",
      "train loss:0.14068166772341364\n",
      "train loss:0.04912729698503315\n",
      "train loss:0.05633783941743088\n",
      "train loss:0.050931085521032496\n",
      "train loss:0.0580841642086158\n",
      "train loss:0.0602268391754336\n",
      "train loss:0.05330520693692363\n",
      "train loss:0.038246441169460625\n",
      "train loss:0.03467792001106091\n",
      "train loss:0.0448957592305332\n",
      "train loss:0.048495317253812856\n",
      "train loss:0.033270981732355605\n",
      "train loss:0.06386426438878376\n",
      "train loss:0.07589178719503784\n",
      "train loss:0.05597271536206188\n",
      "train loss:0.03233634272774275\n",
      "train loss:0.07041910648797059\n",
      "train loss:0.0569438286156523\n",
      "train loss:0.05817538582290856\n",
      "train loss:0.06114322526885075\n",
      "train loss:0.10998774108663387\n",
      "train loss:0.10376410988117428\n",
      "train loss:0.04501136944083336\n",
      "train loss:0.0649365242355145\n",
      "train loss:0.08627140933457397\n",
      "train loss:0.051905802030397845\n",
      "train loss:0.09363537212563403\n",
      "train loss:0.2524191140418915\n",
      "train loss:0.11607297624577098\n",
      "train loss:0.0447892398394109\n",
      "train loss:0.06361787161868374\n",
      "train loss:0.04923119628124231\n",
      "train loss:0.060169156503280065\n",
      "train loss:0.10339175683735329\n",
      "train loss:0.06782742626602203\n",
      "train loss:0.03468574004558934\n",
      "train loss:0.11035861688927225\n",
      "train loss:0.061060786668951546\n",
      "train loss:0.04854681498156955\n",
      "train loss:0.05663160803731879\n",
      "train loss:0.04731023399439469\n",
      "train loss:0.03466782866823414\n",
      "train loss:0.10599201293036137\n",
      "train loss:0.03525771677885139\n",
      "train loss:0.043548258349577214\n",
      "train loss:0.03381403197538028\n",
      "train loss:0.09117613907415581\n",
      "=== epoch:3, train acc:0.979, test acc:0.978 ===\n",
      "train loss:0.06706822572484408\n",
      "train loss:0.10802386210810672\n",
      "train loss:0.10659384918197101\n",
      "train loss:0.09890503122069191\n",
      "train loss:0.11228327383481136\n",
      "train loss:0.04352034134983985\n",
      "train loss:0.03387159319286878\n",
      "train loss:0.06485159586481443\n",
      "train loss:0.030653793762780098\n",
      "train loss:0.07262946677640116\n",
      "train loss:0.13325891012667873\n",
      "train loss:0.0630726358524956\n",
      "train loss:0.03592814347219828\n",
      "train loss:0.028607681871742965\n",
      "train loss:0.05792538414582673\n",
      "train loss:0.046043030425287436\n",
      "train loss:0.07373538590360637\n",
      "train loss:0.02492428233734902\n",
      "train loss:0.07121675428341108\n",
      "train loss:0.07549428171508671\n",
      "train loss:0.08018376736671122\n",
      "train loss:0.026680516403616652\n",
      "train loss:0.05868582096930912\n",
      "train loss:0.09044434064198188\n",
      "train loss:0.049801728296054296\n",
      "train loss:0.024036396159393944\n",
      "train loss:0.04237615751483731\n",
      "train loss:0.05211905060224125\n",
      "train loss:0.05197710673415808\n",
      "train loss:0.030043762474264494\n",
      "train loss:0.05265048969479499\n",
      "train loss:0.062483088199987345\n",
      "train loss:0.10224471119237527\n",
      "train loss:0.04372339174343349\n",
      "train loss:0.13998657944790793\n",
      "train loss:0.13941799538908345\n",
      "train loss:0.017460594010842743\n",
      "train loss:0.05746659938724813\n",
      "train loss:0.07054817082622361\n",
      "train loss:0.060292180867124075\n",
      "train loss:0.047691607877206596\n",
      "train loss:0.0476435469997024\n",
      "train loss:0.09292805104580529\n",
      "train loss:0.13754956871757176\n",
      "train loss:0.0717520128831886\n",
      "train loss:0.10374850499723946\n",
      "train loss:0.07239583111959959\n",
      "train loss:0.04862822978273242\n",
      "train loss:0.10052107442509266\n",
      "train loss:0.053886537690889344\n",
      "train loss:0.05235736061738857\n",
      "train loss:0.11597153619833486\n",
      "train loss:0.04347584425307673\n",
      "train loss:0.13026915086099702\n",
      "train loss:0.1111249671304003\n",
      "train loss:0.03613416937491607\n",
      "train loss:0.06292818022944782\n",
      "train loss:0.06391373495725146\n",
      "train loss:0.02543228976661332\n",
      "train loss:0.03706849416683931\n",
      "train loss:0.08035541067966953\n",
      "train loss:0.07870982776212183\n",
      "train loss:0.038565085311328684\n",
      "train loss:0.020653539924940626\n",
      "train loss:0.051059847008641884\n",
      "train loss:0.035494202545250896\n",
      "train loss:0.021054725010956864\n",
      "train loss:0.022193931015847674\n",
      "train loss:0.05265306493297036\n",
      "train loss:0.07043915597450465\n",
      "train loss:0.02212675661112378\n",
      "train loss:0.05135328342020296\n",
      "train loss:0.06811295202860357\n",
      "train loss:0.09038480161258247\n",
      "train loss:0.08853061839842975\n",
      "train loss:0.12205621588490677\n",
      "train loss:0.0352164349425274\n",
      "train loss:0.1115701666573768\n",
      "train loss:0.08280292872901317\n",
      "train loss:0.06101663254362118\n",
      "train loss:0.035018512243323204\n",
      "train loss:0.07899211342054267\n",
      "train loss:0.0705216356336382\n",
      "train loss:0.031079657116650533\n",
      "train loss:0.022194763917958223\n",
      "train loss:0.046188043500640354\n",
      "train loss:0.09619614972005527\n",
      "train loss:0.015412916306579488\n",
      "train loss:0.048231206275836415\n",
      "train loss:0.051295906045038316\n",
      "train loss:0.028330097590810704\n",
      "train loss:0.05898065985928256\n",
      "train loss:0.07954452729886735\n",
      "train loss:0.06063952575331303\n",
      "train loss:0.07074880961644554\n",
      "train loss:0.028527602126401375\n",
      "train loss:0.07218279082451445\n",
      "train loss:0.07302826913882965\n",
      "train loss:0.15550798192758944\n",
      "train loss:0.026928999937747597\n",
      "train loss:0.05436136294191299\n",
      "train loss:0.0778276689007293\n",
      "train loss:0.10009648868794949\n",
      "train loss:0.08529660250556692\n",
      "train loss:0.016359204287882984\n",
      "train loss:0.07980070977754294\n",
      "train loss:0.030995777193217174\n",
      "train loss:0.09219489921920575\n",
      "train loss:0.02010007114357926\n",
      "train loss:0.11975311594002722\n",
      "train loss:0.03349185437696135\n",
      "train loss:0.17332475393278543\n",
      "train loss:0.0466575246091388\n",
      "train loss:0.058611496745500515\n",
      "train loss:0.030671323173302695\n",
      "train loss:0.026093791591319047\n",
      "train loss:0.04765123900091879\n",
      "train loss:0.09537429887241984\n",
      "train loss:0.06034479347680983\n",
      "train loss:0.02760974342676804\n",
      "train loss:0.029518782419153736\n",
      "train loss:0.038365553332721576\n",
      "train loss:0.05517429854106279\n",
      "train loss:0.034099236587473436\n",
      "train loss:0.03443602846919148\n",
      "train loss:0.02535725843791194\n",
      "train loss:0.12887838833982088\n",
      "train loss:0.03181010668735569\n",
      "train loss:0.06124212514601122\n",
      "train loss:0.013571567149119638\n",
      "train loss:0.05221729899756123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0827501437670402\n",
      "train loss:0.05213284410481341\n",
      "train loss:0.07077826805332639\n",
      "train loss:0.008898052265397938\n",
      "train loss:0.030284752055743446\n",
      "train loss:0.0233720404762211\n",
      "train loss:0.08589317874424779\n",
      "train loss:0.09792980096803985\n",
      "train loss:0.07403071265485882\n",
      "train loss:0.04199794427625995\n",
      "train loss:0.026193238815843477\n",
      "train loss:0.08149509664925944\n",
      "train loss:0.09522900857281485\n",
      "train loss:0.03592318185284538\n",
      "train loss:0.02279476058122791\n",
      "train loss:0.09800333009804219\n",
      "train loss:0.08458008827679857\n",
      "train loss:0.09357874819647993\n",
      "train loss:0.04457356570836442\n",
      "train loss:0.03724751932143049\n",
      "train loss:0.03211247854953301\n",
      "train loss:0.07086097770738423\n",
      "train loss:0.057412872172310295\n",
      "train loss:0.05081047902502828\n",
      "train loss:0.08608513816982556\n",
      "train loss:0.06426085915384905\n",
      "train loss:0.022005666613549583\n",
      "train loss:0.06609828073375412\n",
      "train loss:0.18676384472735916\n",
      "train loss:0.06256636196332314\n",
      "train loss:0.015368588787676031\n",
      "train loss:0.019434180334283804\n",
      "train loss:0.06827105698471697\n",
      "train loss:0.05660991317029551\n",
      "train loss:0.10297555525750969\n",
      "train loss:0.1561197209020099\n",
      "train loss:0.04247678882121498\n",
      "train loss:0.021080029371835632\n",
      "train loss:0.02603117143699486\n",
      "train loss:0.0443779125354576\n",
      "train loss:0.06699417646041116\n",
      "train loss:0.048666866048868204\n",
      "train loss:0.03909774888045109\n",
      "train loss:0.03509451380237947\n",
      "train loss:0.08320967663718426\n",
      "train loss:0.013449821997186952\n",
      "train loss:0.13170989861976956\n",
      "train loss:0.03599771754331283\n",
      "train loss:0.06049712804060899\n",
      "train loss:0.040071751076167256\n",
      "train loss:0.06527011834892106\n",
      "train loss:0.048180273922544876\n",
      "train loss:0.04652818053354867\n",
      "train loss:0.13924252348946067\n",
      "train loss:0.019907848329246074\n",
      "train loss:0.0644989047900851\n",
      "train loss:0.028175461498998532\n",
      "train loss:0.10290814299581509\n",
      "train loss:0.07594769415315572\n",
      "train loss:0.037853564093364385\n",
      "train loss:0.032888374390826015\n",
      "train loss:0.08088709682021485\n",
      "train loss:0.05627904800201837\n",
      "train loss:0.07411045241601709\n",
      "train loss:0.0319616048689408\n",
      "train loss:0.02145726666672334\n",
      "train loss:0.043557579851448425\n",
      "train loss:0.13016429749946518\n",
      "train loss:0.05192924140019481\n",
      "train loss:0.05786460095641768\n",
      "train loss:0.023372020069378885\n",
      "train loss:0.03134856348661416\n",
      "train loss:0.025837297853973182\n",
      "train loss:0.07109601666603152\n",
      "train loss:0.015448173518122084\n",
      "train loss:0.07468976982079538\n",
      "train loss:0.058272381040201014\n",
      "train loss:0.02015635865158419\n",
      "train loss:0.012196393527836675\n",
      "train loss:0.036111467945221604\n",
      "train loss:0.013689626042359205\n",
      "train loss:0.04962992333124228\n",
      "train loss:0.04006396422268444\n",
      "train loss:0.04449177516541683\n",
      "train loss:0.04730767185098452\n",
      "train loss:0.029793169711935616\n",
      "train loss:0.08006786153282333\n",
      "train loss:0.023350715332585873\n",
      "train loss:0.060637364857543806\n",
      "train loss:0.028798489412439014\n",
      "train loss:0.07647945367099847\n",
      "train loss:0.03613018914364233\n",
      "train loss:0.07519386002127044\n",
      "train loss:0.04793155757560764\n",
      "train loss:0.09780899048115856\n",
      "train loss:0.099868058989039\n",
      "train loss:0.04245725888968937\n",
      "train loss:0.04665464480707042\n",
      "train loss:0.0416496873233007\n",
      "train loss:0.05967884245286583\n",
      "train loss:0.06620102011166316\n",
      "train loss:0.04341679074676863\n",
      "train loss:0.017925480057624333\n",
      "train loss:0.08232926071586119\n",
      "train loss:0.026640931454927092\n",
      "train loss:0.01888433224955212\n",
      "train loss:0.087367813675012\n",
      "train loss:0.12721092191851238\n",
      "train loss:0.10435189708271238\n",
      "train loss:0.05034135548680927\n",
      "train loss:0.039513458331501064\n",
      "train loss:0.03979539238232766\n",
      "train loss:0.021190107314869963\n",
      "train loss:0.1273976314863205\n",
      "train loss:0.07249304099769142\n",
      "train loss:0.054707506125959496\n",
      "train loss:0.03758694992162672\n",
      "train loss:0.04999149818373084\n",
      "train loss:0.025884787509312324\n",
      "train loss:0.04083768363791665\n",
      "train loss:0.049042412911387646\n",
      "train loss:0.0431161223553678\n",
      "train loss:0.02473424729068943\n",
      "train loss:0.031291576197322535\n",
      "train loss:0.051341652232295774\n",
      "train loss:0.08840735900722244\n",
      "train loss:0.014729668348178898\n",
      "train loss:0.03830702736472252\n",
      "train loss:0.04576430356517998\n",
      "train loss:0.0849309792713358\n",
      "train loss:0.06590327966870671\n",
      "train loss:0.06864217531122105\n",
      "train loss:0.010628499125534272\n",
      "train loss:0.013803732260436949\n",
      "train loss:0.07314917371015843\n",
      "train loss:0.017015281922922322\n",
      "train loss:0.04608136633445729\n",
      "train loss:0.029453328805852323\n",
      "train loss:0.10192352733593828\n",
      "train loss:0.10846813541995629\n",
      "train loss:0.05335566851694991\n",
      "train loss:0.040517093465047116\n",
      "train loss:0.03344152077287073\n",
      "train loss:0.023579347476527422\n",
      "train loss:0.12737453254784353\n",
      "train loss:0.08652486532050466\n",
      "train loss:0.08649966520187316\n",
      "train loss:0.02669423295073322\n",
      "train loss:0.05224786885535285\n",
      "train loss:0.012831721842426081\n",
      "train loss:0.03253782128204226\n",
      "train loss:0.038338239176638804\n",
      "train loss:0.0926225999488381\n",
      "train loss:0.06014182111363273\n",
      "train loss:0.10190532259130494\n",
      "train loss:0.08437400098959143\n",
      "train loss:0.11361156844493996\n",
      "train loss:0.04762251392998555\n",
      "train loss:0.07476687070905737\n",
      "train loss:0.027651080904605888\n",
      "train loss:0.06669080964515735\n",
      "train loss:0.057129618709667825\n",
      "train loss:0.036378058415486\n",
      "train loss:0.04461435271511334\n",
      "train loss:0.07568930874898214\n",
      "train loss:0.05939507845902188\n",
      "train loss:0.03408612913168746\n",
      "train loss:0.04505569379980965\n",
      "train loss:0.03319276847198605\n",
      "train loss:0.04565944686322409\n",
      "train loss:0.07843806515446626\n",
      "train loss:0.02086136624718639\n",
      "train loss:0.021191921062554048\n",
      "train loss:0.0734438756858912\n",
      "train loss:0.0523182428954045\n",
      "train loss:0.09273845352573912\n",
      "train loss:0.09488738530306877\n",
      "train loss:0.04589168342175923\n",
      "train loss:0.030208990889210627\n",
      "train loss:0.04538746171331223\n",
      "train loss:0.06504798443848481\n",
      "train loss:0.031046839235250506\n",
      "train loss:0.07754856573358829\n",
      "train loss:0.03847667709209967\n",
      "train loss:0.06453353738229337\n",
      "train loss:0.06325859243054363\n",
      "train loss:0.053652840839961684\n",
      "train loss:0.0856923708732691\n",
      "train loss:0.058883973639720616\n",
      "train loss:0.05378277119968504\n",
      "train loss:0.11543937059378956\n",
      "train loss:0.05678962635352118\n",
      "train loss:0.06531780462010243\n",
      "train loss:0.04866363860176839\n",
      "train loss:0.023639717432789434\n",
      "train loss:0.035481449265786794\n",
      "train loss:0.03975799264031305\n",
      "train loss:0.02269600670311399\n",
      "train loss:0.041006254250584975\n",
      "train loss:0.03922363168218147\n",
      "train loss:0.08333469990706391\n",
      "train loss:0.009514310384150007\n",
      "train loss:0.02187821658403533\n",
      "train loss:0.032206307371370105\n",
      "train loss:0.02776590647277448\n",
      "train loss:0.05126394132245797\n",
      "train loss:0.04367508414591481\n",
      "train loss:0.03344094500171511\n",
      "train loss:0.03266600381679064\n",
      "train loss:0.02902110673945092\n",
      "train loss:0.036678672793786254\n",
      "train loss:0.026213645615632334\n",
      "train loss:0.06141028040323536\n",
      "train loss:0.08145094493827512\n",
      "train loss:0.02817301678719856\n",
      "train loss:0.10296088192555722\n",
      "train loss:0.0480770280908903\n",
      "train loss:0.0382651144479138\n",
      "train loss:0.033546140959792005\n",
      "train loss:0.018304048632098745\n",
      "train loss:0.04950845192076768\n",
      "train loss:0.03302649815704656\n",
      "train loss:0.020359957282657635\n",
      "train loss:0.019939175805381862\n",
      "train loss:0.02750419754214827\n",
      "train loss:0.01085394653193806\n",
      "train loss:0.013423421299702962\n",
      "train loss:0.034854683342132285\n",
      "train loss:0.050944919272851\n",
      "train loss:0.10843432017120067\n",
      "train loss:0.06008105880509103\n",
      "train loss:0.02268795744018012\n",
      "train loss:0.044886094649213136\n",
      "train loss:0.03044076805255098\n",
      "train loss:0.04685425342718973\n",
      "train loss:0.10342707449905092\n",
      "train loss:0.0700445024142474\n",
      "train loss:0.052354041196288656\n",
      "train loss:0.06360782740571586\n",
      "train loss:0.06246403870714754\n",
      "train loss:0.061930043531920075\n",
      "train loss:0.022945526003742854\n",
      "train loss:0.03958305462564003\n",
      "train loss:0.04434623389605338\n",
      "train loss:0.02151080815334881\n",
      "train loss:0.0775365826260244\n",
      "train loss:0.024536199620989362\n",
      "train loss:0.04139436976313621\n",
      "train loss:0.018503835787104354\n",
      "train loss:0.0160815223977527\n",
      "train loss:0.08995787594353055\n",
      "train loss:0.05950541071208807\n",
      "train loss:0.03421009552063988\n",
      "train loss:0.03844314797347955\n",
      "train loss:0.05138046713873739\n",
      "train loss:0.05908651343217079\n",
      "train loss:0.04804426915017708\n",
      "train loss:0.022967796429668664\n",
      "train loss:0.06745163074792927\n",
      "train loss:0.1202055129092286\n",
      "train loss:0.07832169481726414\n",
      "train loss:0.056134023886234355\n",
      "train loss:0.09415355768180939\n",
      "train loss:0.03224819274635927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.015851199824065455\n",
      "train loss:0.06172424162565402\n",
      "train loss:0.07113548012970464\n",
      "train loss:0.06555895865974355\n",
      "train loss:0.022092807921356598\n",
      "train loss:0.06643387676090709\n",
      "train loss:0.052009636758311446\n",
      "train loss:0.032466037386269006\n",
      "train loss:0.025135939363477414\n",
      "train loss:0.05728276446359744\n",
      "train loss:0.11388323510196412\n",
      "train loss:0.024127063903999345\n",
      "train loss:0.02795204402961724\n",
      "train loss:0.08146143615679295\n",
      "train loss:0.022141612488029424\n",
      "train loss:0.021090938444002724\n",
      "train loss:0.033035201701656684\n",
      "train loss:0.014050066577989446\n",
      "train loss:0.027642746833001566\n",
      "train loss:0.11979129825975227\n",
      "train loss:0.03739555259865543\n",
      "train loss:0.05894599605031654\n",
      "train loss:0.02160057657204324\n",
      "train loss:0.1323113708528236\n",
      "train loss:0.04025153109737976\n",
      "train loss:0.045718360043388404\n",
      "train loss:0.06957395998184493\n",
      "train loss:0.06444574564287216\n",
      "train loss:0.08247459986717348\n",
      "train loss:0.029200710930406836\n",
      "train loss:0.015536738731650488\n",
      "train loss:0.07580955147236736\n",
      "train loss:0.054147863321779344\n",
      "train loss:0.015467498050253065\n",
      "train loss:0.07395063201246592\n",
      "train loss:0.031407817220220374\n",
      "train loss:0.05818814769957356\n",
      "train loss:0.029617176884029622\n",
      "train loss:0.046378997314089745\n",
      "train loss:0.02669060003608896\n",
      "train loss:0.14151340040637314\n",
      "train loss:0.05526239813864238\n",
      "train loss:0.02072529592102048\n",
      "train loss:0.0195700359051151\n",
      "train loss:0.03281953252708735\n",
      "train loss:0.14670033416015538\n",
      "train loss:0.09607209080625244\n",
      "train loss:0.028912692385852746\n",
      "train loss:0.09055689445320934\n",
      "train loss:0.019372028055739513\n",
      "train loss:0.059047063419839736\n",
      "train loss:0.04052318504853567\n",
      "train loss:0.010423669639697042\n",
      "train loss:0.029838090713333022\n",
      "train loss:0.04423522242520427\n",
      "train loss:0.03575936963259514\n",
      "train loss:0.03668037397671168\n",
      "train loss:0.12138677809901736\n",
      "train loss:0.03073893143325381\n",
      "train loss:0.05699706683489079\n",
      "train loss:0.04032055505013047\n",
      "train loss:0.06301406722875183\n",
      "train loss:0.030792424893100825\n",
      "train loss:0.040522662749792424\n",
      "train loss:0.05455153279572123\n",
      "train loss:0.1420514751847621\n",
      "train loss:0.051255355654363226\n",
      "train loss:0.03624715474859053\n",
      "train loss:0.019199191109831565\n",
      "train loss:0.027292508407283478\n",
      "train loss:0.017071038891373867\n",
      "train loss:0.05108177424225325\n",
      "train loss:0.03169701963110572\n",
      "train loss:0.03664334568521013\n",
      "train loss:0.03814889788052085\n",
      "train loss:0.07210710396853626\n",
      "train loss:0.03181109513011413\n",
      "train loss:0.0815337814914757\n",
      "train loss:0.019738844775055612\n",
      "train loss:0.11726037132669118\n",
      "train loss:0.018018826935182693\n",
      "train loss:0.014180218662154966\n",
      "train loss:0.07328999850513873\n",
      "train loss:0.051942456451961515\n",
      "train loss:0.04106459322265338\n",
      "train loss:0.04823600649687564\n",
      "train loss:0.026650977895437737\n",
      "train loss:0.05937448632018837\n",
      "train loss:0.03458266423650297\n",
      "train loss:0.035120581966907406\n",
      "train loss:0.027323213835835553\n",
      "train loss:0.050429242132823245\n",
      "train loss:0.06874733464140816\n",
      "train loss:0.06486326512358365\n",
      "train loss:0.04724202822954937\n",
      "train loss:0.01618524725870272\n",
      "train loss:0.03807563041977138\n",
      "train loss:0.03435057850277619\n",
      "train loss:0.06509518630813918\n",
      "train loss:0.02319395172888515\n",
      "train loss:0.054194204845824975\n",
      "train loss:0.014214703177595562\n",
      "train loss:0.06408211829343508\n",
      "train loss:0.0771893442662249\n",
      "train loss:0.05125652904912656\n",
      "train loss:0.05929530510784233\n",
      "train loss:0.023815872846755334\n",
      "train loss:0.04259974476417381\n",
      "train loss:0.05233381843255344\n",
      "train loss:0.06976257031199772\n",
      "train loss:0.028680375738319105\n",
      "train loss:0.08879500843984532\n",
      "train loss:0.04230231236575117\n",
      "train loss:0.03487227900297364\n",
      "train loss:0.06838114212042941\n",
      "train loss:0.02606977898802892\n",
      "train loss:0.06846278202852055\n",
      "train loss:0.09650245152428598\n",
      "train loss:0.031676186218362753\n",
      "train loss:0.011572805534899205\n",
      "train loss:0.08511872416156416\n",
      "train loss:0.1447275485339921\n",
      "train loss:0.0307462883698005\n",
      "train loss:0.02069178475499756\n",
      "train loss:0.02661026930392072\n",
      "train loss:0.03445493433066743\n",
      "train loss:0.03679276680047975\n",
      "train loss:0.03055807085864207\n",
      "train loss:0.05903928436483428\n",
      "train loss:0.04545128352538276\n",
      "train loss:0.053625243780848514\n",
      "train loss:0.022018253409643584\n",
      "train loss:0.02744567736775373\n",
      "train loss:0.06740729218123845\n",
      "train loss:0.06652258457525345\n",
      "train loss:0.04554726200605657\n",
      "train loss:0.03957214618105436\n",
      "train loss:0.017754122866256227\n",
      "train loss:0.009578225829906776\n",
      "train loss:0.033210277363017154\n",
      "train loss:0.10724426411437919\n",
      "train loss:0.04558107141707691\n",
      "train loss:0.07760290712003864\n",
      "train loss:0.0719710350532754\n",
      "train loss:0.03619005864406438\n",
      "train loss:0.08228274458295753\n",
      "train loss:0.03350285478352697\n",
      "train loss:0.010749411982021886\n",
      "train loss:0.04551300739249019\n",
      "train loss:0.03506275923813576\n",
      "train loss:0.060032070059954964\n",
      "train loss:0.0975768070584837\n",
      "train loss:0.02386835390621475\n",
      "train loss:0.08764393194225852\n",
      "train loss:0.04371607751152418\n",
      "train loss:0.010613172120932572\n",
      "train loss:0.05261253082952424\n",
      "train loss:0.02932291279110225\n",
      "train loss:0.024553553184290266\n",
      "train loss:0.03551763157261007\n",
      "train loss:0.03330473759048956\n",
      "train loss:0.036076423763441126\n",
      "train loss:0.013946842509836582\n",
      "train loss:0.03205557525942755\n",
      "train loss:0.017771852847215684\n",
      "train loss:0.03203854458063072\n",
      "train loss:0.07781813302974194\n",
      "train loss:0.024909396985336528\n",
      "train loss:0.02491527991252577\n",
      "train loss:0.007847667345493942\n",
      "train loss:0.023612543393222992\n",
      "train loss:0.01607560598526075\n",
      "train loss:0.034873331201672186\n",
      "train loss:0.010844856010679815\n",
      "train loss:0.058081595595788904\n",
      "train loss:0.031801590773209897\n",
      "train loss:0.05316732992825826\n",
      "train loss:0.09853110033334046\n",
      "train loss:0.028542028936984844\n",
      "train loss:0.03371983180935823\n",
      "train loss:0.03953263485938334\n",
      "train loss:0.10416408676195149\n",
      "train loss:0.04993066308505877\n",
      "train loss:0.054781905701095306\n",
      "train loss:0.06055689390667204\n",
      "train loss:0.05579726851939855\n",
      "train loss:0.02670823803962423\n",
      "train loss:0.07990165420554483\n",
      "train loss:0.18904260303701353\n",
      "train loss:0.08432432750883903\n",
      "train loss:0.06255315987904539\n",
      "train loss:0.05928189596145472\n",
      "train loss:0.03424938552444742\n",
      "train loss:0.09784228163844473\n",
      "train loss:0.02978723977758977\n",
      "train loss:0.022077470039203496\n",
      "train loss:0.035987318827516414\n",
      "train loss:0.024099660063882936\n",
      "train loss:0.08357513912323798\n",
      "train loss:0.02742530384076734\n",
      "train loss:0.03527535792405321\n",
      "train loss:0.05654170577937097\n",
      "train loss:0.021375945432512755\n",
      "train loss:0.05098328559989759\n",
      "train loss:0.03597793018006596\n",
      "=== epoch:4, train acc:0.985, test acc:0.979 ===\n",
      "train loss:0.18239197811979369\n",
      "train loss:0.07912176924061551\n",
      "train loss:0.02659906843497731\n",
      "train loss:0.02190112576424663\n",
      "train loss:0.051346039817287255\n",
      "train loss:0.0717636132183824\n",
      "train loss:0.03532531194487035\n",
      "train loss:0.05800790314539368\n",
      "train loss:0.05356587045663923\n",
      "train loss:0.021901513076619188\n",
      "train loss:0.0777448171783583\n",
      "train loss:0.08347098549936824\n",
      "train loss:0.1088932221523263\n",
      "train loss:0.08912583460844223\n",
      "train loss:0.012226737011064059\n",
      "train loss:0.025166411145600334\n",
      "train loss:0.037806107165641215\n",
      "train loss:0.038540374134534615\n",
      "train loss:0.022270957616499207\n",
      "train loss:0.03342213248126108\n",
      "train loss:0.006994017508225676\n",
      "train loss:0.02826602042844019\n",
      "train loss:0.039117822775806145\n",
      "train loss:0.04111710877121471\n",
      "train loss:0.06102188270592797\n",
      "train loss:0.02178825792777901\n",
      "train loss:0.044926484271859815\n",
      "train loss:0.13729161219275615\n",
      "train loss:0.016879316851902356\n",
      "train loss:0.04581469441571618\n",
      "train loss:0.05109561916899057\n",
      "train loss:0.025412987644631978\n",
      "train loss:0.05474885522729135\n",
      "train loss:0.037952875265763825\n",
      "train loss:0.03120375826028495\n",
      "train loss:0.03463362682413745\n",
      "train loss:0.06348997720897158\n",
      "train loss:0.027888874684283967\n",
      "train loss:0.04466571296860481\n",
      "train loss:0.05593898281155722\n",
      "train loss:0.07571303718112005\n",
      "train loss:0.011519688972350547\n",
      "train loss:0.014895836310719952\n",
      "train loss:0.022429460157609774\n",
      "train loss:0.034661441097785864\n",
      "train loss:0.028684131349353038\n",
      "train loss:0.029450696492297212\n",
      "train loss:0.046045420648173734\n",
      "train loss:0.03201444105913502\n",
      "train loss:0.0389509256285765\n",
      "train loss:0.02298164486593819\n",
      "train loss:0.05852110743902784\n",
      "train loss:0.16959827612522016\n",
      "train loss:0.019497550959110103\n",
      "train loss:0.0507379400574896\n",
      "train loss:0.1495077714014373\n",
      "train loss:0.06595847132824899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04403818421544336\n",
      "train loss:0.04005469816144879\n",
      "train loss:0.06313285586432349\n",
      "train loss:0.014237928662246673\n",
      "train loss:0.035824227775441225\n",
      "train loss:0.030156560370673793\n",
      "train loss:0.1315259034505982\n",
      "train loss:0.03152630185020201\n",
      "train loss:0.02980038681899791\n",
      "train loss:0.023668301291966107\n",
      "train loss:0.03116185367813889\n",
      "train loss:0.02961604694982318\n",
      "train loss:0.014439579712198504\n",
      "train loss:0.032766600600768955\n",
      "train loss:0.04539202684806644\n",
      "train loss:0.05038927799771865\n",
      "train loss:0.07695910886504366\n",
      "train loss:0.014287174666690034\n",
      "train loss:0.04192369279402368\n",
      "train loss:0.019607282200423196\n",
      "train loss:0.043856019337991325\n",
      "train loss:0.009630540561007955\n",
      "train loss:0.03505579753922553\n",
      "train loss:0.07554884830356783\n",
      "train loss:0.07756168143224075\n",
      "train loss:0.03646119391713797\n",
      "train loss:0.09817301304699413\n",
      "train loss:0.09519875560677785\n",
      "train loss:0.06572426558201733\n",
      "train loss:0.013482569954285166\n",
      "train loss:0.04999663475495535\n",
      "train loss:0.02255532882486314\n",
      "train loss:0.03786468334361379\n",
      "train loss:0.006750602892972451\n",
      "train loss:0.016991187889306068\n",
      "train loss:0.014569148513444959\n",
      "train loss:0.07131938929692858\n",
      "train loss:0.034330561424173116\n",
      "train loss:0.10895193820630465\n",
      "train loss:0.023137748411541578\n",
      "train loss:0.10880407871508371\n",
      "train loss:0.057932817823627074\n",
      "train loss:0.053424701113001044\n",
      "train loss:0.03290789028572535\n",
      "train loss:0.03661057747819585\n",
      "train loss:0.011512547532140417\n",
      "train loss:0.04128424765180306\n",
      "train loss:0.05734591514978708\n",
      "train loss:0.008346639577178716\n",
      "train loss:0.012784856529576123\n",
      "train loss:0.03901018676614294\n",
      "train loss:0.0752376637715495\n",
      "train loss:0.06710849212577157\n",
      "train loss:0.035731968782133486\n",
      "train loss:0.020123733789314938\n",
      "train loss:0.048874784618299055\n",
      "train loss:0.032755713905062774\n",
      "train loss:0.022015725924358364\n",
      "train loss:0.038238620254371275\n",
      "train loss:0.03193854544081501\n",
      "train loss:0.02175306072958429\n",
      "train loss:0.04711818230309788\n",
      "train loss:0.012965457911571474\n",
      "train loss:0.038179785263425664\n",
      "train loss:0.04069540074800754\n",
      "train loss:0.01935121533391045\n",
      "train loss:0.051430443469306544\n",
      "train loss:0.03579593651281422\n",
      "train loss:0.01572008728930074\n",
      "train loss:0.0647773262071858\n",
      "train loss:0.013063209861447978\n",
      "train loss:0.07889930079248589\n",
      "train loss:0.042583025003904915\n",
      "train loss:0.034754190727572104\n",
      "train loss:0.03229351478424394\n",
      "train loss:0.010194803096288554\n",
      "train loss:0.10718837733436706\n",
      "train loss:0.01844002026258645\n",
      "train loss:0.04464822209371157\n",
      "train loss:0.048945779742450526\n",
      "train loss:0.027111085945834324\n",
      "train loss:0.037034243896469914\n",
      "train loss:0.03122601954521656\n",
      "train loss:0.029363417970532\n",
      "train loss:0.051525937425612574\n",
      "train loss:0.01570288541392402\n",
      "train loss:0.019290491510481816\n",
      "train loss:0.029718707703407687\n",
      "train loss:0.03787699707322773\n",
      "train loss:0.027663649500719863\n",
      "train loss:0.03351359797401046\n",
      "train loss:0.012626458400651398\n",
      "train loss:0.0122644387125556\n",
      "train loss:0.01322665349166537\n",
      "train loss:0.0697215815926247\n",
      "train loss:0.08260263592041049\n",
      "train loss:0.03582250526800303\n",
      "train loss:0.022086679988818192\n",
      "train loss:0.029782306026180986\n",
      "train loss:0.034589884939845716\n",
      "train loss:0.021753937011341318\n",
      "train loss:0.013917458958715817\n",
      "train loss:0.08351895910576253\n",
      "train loss:0.016669755920146457\n",
      "train loss:0.05423580302158142\n",
      "train loss:0.013159622200739912\n",
      "train loss:0.12439256561795388\n",
      "train loss:0.09715465353363965\n",
      "train loss:0.06314824081389384\n",
      "train loss:0.0368564768967426\n",
      "train loss:0.045674539989567116\n",
      "train loss:0.04133143409587304\n",
      "train loss:0.009121140629925618\n",
      "train loss:0.12213939017624989\n",
      "train loss:0.028633402180094372\n",
      "train loss:0.03316710013769399\n",
      "train loss:0.061867007788692883\n",
      "train loss:0.06038491536541775\n",
      "train loss:0.06547959487949571\n",
      "train loss:0.03591689746798421\n",
      "train loss:0.03531139585617405\n",
      "train loss:0.056505814488866796\n",
      "train loss:0.06905635813464156\n",
      "train loss:0.03567708106747664\n",
      "train loss:0.03202546064628433\n",
      "train loss:0.10747562544538228\n",
      "train loss:0.018622717114517684\n",
      "train loss:0.03456216996519584\n",
      "train loss:0.01791004507787376\n",
      "train loss:0.04159392927752698\n",
      "train loss:0.05222016506829596\n",
      "train loss:0.07152713006081121\n",
      "train loss:0.006563744298451978\n",
      "train loss:0.13635110996793687\n",
      "train loss:0.018091714794660686\n",
      "train loss:0.023440355394468336\n",
      "train loss:0.048749614437012825\n",
      "train loss:0.020323282816226168\n",
      "train loss:0.02831217490397391\n",
      "train loss:0.04550830025099521\n",
      "train loss:0.046618291402091436\n",
      "train loss:0.01856035632927844\n",
      "train loss:0.027754342372697458\n",
      "train loss:0.023544878342045925\n",
      "train loss:0.015361352751117736\n",
      "train loss:0.0586587608049013\n",
      "train loss:0.037239792037721056\n",
      "train loss:0.026182674552298148\n",
      "train loss:0.006933039295272726\n",
      "train loss:0.023229000819057613\n",
      "train loss:0.033017835889385616\n",
      "train loss:0.0390699463559296\n",
      "train loss:0.015648259852596418\n",
      "train loss:0.03622103040753592\n",
      "train loss:0.029447228310493716\n",
      "train loss:0.0833590299024825\n",
      "train loss:0.030827439887398948\n",
      "train loss:0.010476453842869007\n",
      "train loss:0.05718660595418335\n",
      "train loss:0.038131226292275444\n",
      "train loss:0.008958658416165177\n",
      "train loss:0.12629730586514107\n",
      "train loss:0.02758061164134086\n",
      "train loss:0.01318106381124546\n",
      "train loss:0.07417909912431574\n",
      "train loss:0.021532182591571115\n",
      "train loss:0.05780201598749815\n",
      "train loss:0.025630567578326987\n",
      "train loss:0.03778701229746837\n",
      "train loss:0.00750381210928442\n",
      "train loss:0.01219751165881974\n",
      "train loss:0.041007258208404763\n",
      "train loss:0.05668243179262551\n",
      "train loss:0.04585792769164596\n",
      "train loss:0.08692575668242161\n",
      "train loss:0.024158527838595446\n",
      "train loss:0.008546042840480291\n",
      "train loss:0.020431538616493147\n",
      "train loss:0.014483188200619654\n",
      "train loss:0.006140139670340752\n",
      "train loss:0.04912219675421082\n",
      "train loss:0.026962153591572596\n",
      "train loss:0.04125854941724114\n",
      "train loss:0.03011390480794711\n",
      "train loss:0.022450569246719308\n",
      "train loss:0.004955381456814115\n",
      "train loss:0.018555438247920362\n",
      "train loss:0.018865774141544323\n",
      "train loss:0.07543931192224074\n",
      "train loss:0.00927175315521285\n",
      "train loss:0.021650365273896948\n",
      "train loss:0.04500999027698253\n",
      "train loss:0.06386983089209618\n",
      "train loss:0.04024993290501164\n",
      "train loss:0.019903124941744742\n",
      "train loss:0.019416503426846452\n",
      "train loss:0.00931460504414187\n",
      "train loss:0.062883377960735\n",
      "train loss:0.02050653984654419\n",
      "train loss:0.005437017971952397\n",
      "train loss:0.07317373405890377\n",
      "train loss:0.01504939850943975\n",
      "train loss:0.019708468211374043\n",
      "train loss:0.03908018786621182\n",
      "train loss:0.013618391541587909\n",
      "train loss:0.01697445703955598\n",
      "train loss:0.011442359497042452\n",
      "train loss:0.04189515753655915\n",
      "train loss:0.033754654946522245\n",
      "train loss:0.031089632819211132\n",
      "train loss:0.06397849318196382\n",
      "train loss:0.04223378324883664\n",
      "train loss:0.012441743387702717\n",
      "train loss:0.07277468291881765\n",
      "train loss:0.012198438537583813\n",
      "train loss:0.03055255326723938\n",
      "train loss:0.03651326013221773\n",
      "train loss:0.05873897677618679\n",
      "train loss:0.06259674422995308\n",
      "train loss:0.05272988696516664\n",
      "train loss:0.010705937460809828\n",
      "train loss:0.017042689737381413\n",
      "train loss:0.016104678843216615\n",
      "train loss:0.1165177144378419\n",
      "train loss:0.07783900263255938\n",
      "train loss:0.03563330043249017\n",
      "train loss:0.03709689649708902\n",
      "train loss:0.027047439601061072\n",
      "train loss:0.10949324141988705\n",
      "train loss:0.020831259408660895\n",
      "train loss:0.013781106168471412\n",
      "train loss:0.08583145311504144\n",
      "train loss:0.03754150087339911\n",
      "train loss:0.0671678371037057\n",
      "train loss:0.009149764987894634\n",
      "train loss:0.03183581412409319\n",
      "train loss:0.024923513348329557\n",
      "train loss:0.018705855990885464\n",
      "train loss:0.01207005600439215\n",
      "train loss:0.020949370719183777\n",
      "train loss:0.035154403979443256\n",
      "train loss:0.02488489363681827\n",
      "train loss:0.023729153683218502\n",
      "train loss:0.04835299624600736\n",
      "train loss:0.05492883815383665\n",
      "train loss:0.037603193204134126\n",
      "train loss:0.030027091580021495\n",
      "train loss:0.011182815402182753\n",
      "train loss:0.019397811016778087\n",
      "train loss:0.018408367160794498\n",
      "train loss:0.03837921588817982\n",
      "train loss:0.042535465079976034\n",
      "train loss:0.020683228807387256\n",
      "train loss:0.016659652739828168\n",
      "train loss:0.005179771446089143\n",
      "train loss:0.0296054205463433\n",
      "train loss:0.034694707072935746\n",
      "train loss:0.0344163838860432\n",
      "train loss:0.020829077821371566\n",
      "train loss:0.012328146226227377\n",
      "train loss:0.07082105426285133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06472213762359774\n",
      "train loss:0.11870164256892965\n",
      "train loss:0.01993548113999873\n",
      "train loss:0.0317929942119661\n",
      "train loss:0.02650666165051252\n",
      "train loss:0.0066349164708342945\n",
      "train loss:0.013455545297269157\n",
      "train loss:0.0960122141700418\n",
      "train loss:0.01321104802946365\n",
      "train loss:0.03231420012891223\n",
      "train loss:0.03331176736485646\n",
      "train loss:0.016445112877869525\n",
      "train loss:0.03772381185222245\n",
      "train loss:0.03989675894362166\n",
      "train loss:0.03750325232830109\n",
      "train loss:0.007882354279136593\n",
      "train loss:0.016446379295338667\n",
      "train loss:0.017607732581215903\n",
      "train loss:0.009077126586998408\n",
      "train loss:0.03937773067705591\n",
      "train loss:0.01806221014368683\n",
      "train loss:0.01239767088305232\n",
      "train loss:0.023396963481394594\n",
      "train loss:0.06851642494445351\n",
      "train loss:0.02456739473017854\n",
      "train loss:0.008799089188859886\n",
      "train loss:0.029978530898829472\n",
      "train loss:0.017869462258532923\n",
      "train loss:0.03739510838562492\n",
      "train loss:0.021079498479751043\n",
      "train loss:0.007997082182956005\n",
      "train loss:0.015909347770384042\n",
      "train loss:0.06326083830342294\n",
      "train loss:0.04451509679864267\n",
      "train loss:0.016553062559540244\n",
      "train loss:0.013711795561238695\n",
      "train loss:0.041554662356311244\n",
      "train loss:0.012038510162633891\n",
      "train loss:0.03342554570489809\n",
      "train loss:0.12983678916181046\n",
      "train loss:0.010778318477026762\n",
      "train loss:0.02599026151834933\n",
      "train loss:0.029933852595255387\n",
      "train loss:0.04024001405496219\n",
      "train loss:0.08342828653331702\n",
      "train loss:0.05494150771511151\n",
      "train loss:0.0491066888421148\n",
      "train loss:0.018588549998446214\n",
      "train loss:0.04725567410547821\n",
      "train loss:0.009114404805456118\n",
      "train loss:0.019159279083630895\n",
      "train loss:0.03544539996195302\n",
      "train loss:0.02741054972139636\n",
      "train loss:0.09694183322833023\n",
      "train loss:0.02366992373898353\n",
      "train loss:0.02535972595451992\n",
      "train loss:0.02361488697957841\n",
      "train loss:0.10966501657405298\n",
      "train loss:0.03558744304311251\n",
      "train loss:0.039467954787717384\n",
      "train loss:0.010103034241451297\n",
      "train loss:0.11832174997599766\n",
      "train loss:0.010292987809481634\n",
      "train loss:0.019891162651775558\n",
      "train loss:0.04322322840054807\n",
      "train loss:0.023381582122685686\n",
      "train loss:0.02868074554620032\n",
      "train loss:0.06613284645743994\n",
      "train loss:0.06200995381987093\n",
      "train loss:0.07887243425646295\n",
      "train loss:0.028770118974364066\n",
      "train loss:0.06358442149653318\n",
      "train loss:0.027284641869739477\n",
      "train loss:0.0034550185638129007\n",
      "train loss:0.05042494654047063\n",
      "train loss:0.02647355589034985\n",
      "train loss:0.07913767029798376\n",
      "train loss:0.019377224531514976\n",
      "train loss:0.024875266540205342\n",
      "train loss:0.03482391486729466\n",
      "train loss:0.025344974033872193\n",
      "train loss:0.08540687192160731\n",
      "train loss:0.054265066854243746\n",
      "train loss:0.05565226427879575\n",
      "train loss:0.010089395863026557\n",
      "train loss:0.029363975391515664\n",
      "train loss:0.004433959951172493\n",
      "train loss:0.0365611824262777\n",
      "train loss:0.0113567796096336\n",
      "train loss:0.02154176421164189\n",
      "train loss:0.010946062282744388\n",
      "train loss:0.025653367093338576\n",
      "train loss:0.009493268136758515\n",
      "train loss:0.0641048380400028\n",
      "train loss:0.024038385276612652\n",
      "train loss:0.10692259018657198\n",
      "train loss:0.06226759112203159\n",
      "train loss:0.04243034570100429\n",
      "train loss:0.019514411110815665\n",
      "train loss:0.029057363370618233\n",
      "train loss:0.06316925911075981\n",
      "train loss:0.015554061255613188\n",
      "train loss:0.028910792768387907\n",
      "train loss:0.028409534735223802\n",
      "train loss:0.2205093805103238\n",
      "train loss:0.0281044896194356\n",
      "train loss:0.02354911668565589\n",
      "train loss:0.03584555958766838\n",
      "train loss:0.016808505510618823\n",
      "train loss:0.0273796633739976\n",
      "train loss:0.00875761338799787\n",
      "train loss:0.06058491611117558\n",
      "train loss:0.009091432220846959\n",
      "train loss:0.04688358143645305\n",
      "train loss:0.030136909158161727\n",
      "train loss:0.03539885826985912\n",
      "train loss:0.0084597403409443\n",
      "train loss:0.03387862494040001\n",
      "train loss:0.013451615739935563\n",
      "train loss:0.03828313711651149\n",
      "train loss:0.03130639808789559\n",
      "train loss:0.0377736422194923\n",
      "train loss:0.1109201168476741\n",
      "train loss:0.05918926038763326\n",
      "train loss:0.03628837735145694\n",
      "train loss:0.033777697512881046\n",
      "train loss:0.0175921843197976\n",
      "train loss:0.019445653418994485\n",
      "train loss:0.02786211309909974\n",
      "train loss:0.011128066160185288\n",
      "train loss:0.045961347671282286\n",
      "train loss:0.10496503472774243\n",
      "train loss:0.021523490081112973\n",
      "train loss:0.016116164314917587\n",
      "train loss:0.008966745667776386\n",
      "train loss:0.09456630536263688\n",
      "train loss:0.05500730480918769\n",
      "train loss:0.01796418393174298\n",
      "train loss:0.006867223859845946\n",
      "train loss:0.04530080789744868\n",
      "train loss:0.02119411642922369\n",
      "train loss:0.025538975442517528\n",
      "train loss:0.020153063338203478\n",
      "train loss:0.08460120439992926\n",
      "train loss:0.024952454745584326\n",
      "train loss:0.01740646005820877\n",
      "train loss:0.08920725074757951\n",
      "train loss:0.07755667556586253\n",
      "train loss:0.022644552184951813\n",
      "train loss:0.0344947461337082\n",
      "train loss:0.06092543916221918\n",
      "train loss:0.09792135581112241\n",
      "train loss:0.10805198350194553\n",
      "train loss:0.062390932154402085\n",
      "train loss:0.02128510874242817\n",
      "train loss:0.03777207402832855\n",
      "train loss:0.02043222971498719\n",
      "train loss:0.032589548518790826\n",
      "train loss:0.02394576992450466\n",
      "train loss:0.03440061655680036\n",
      "train loss:0.06905134074912299\n",
      "train loss:0.09080066738748775\n",
      "train loss:0.045665704612549686\n",
      "train loss:0.0715852027018984\n",
      "train loss:0.13982322383405182\n",
      "train loss:0.06298739720196496\n",
      "train loss:0.028362230816629953\n",
      "train loss:0.02196587503697391\n",
      "train loss:0.01051986494166702\n",
      "train loss:0.018961574726409334\n",
      "train loss:0.0735937532797551\n",
      "train loss:0.06845621410943992\n",
      "train loss:0.012356599182467476\n",
      "train loss:0.015212369643503428\n",
      "train loss:0.014760538665200607\n",
      "train loss:0.04781131923790693\n",
      "train loss:0.023012597038333076\n",
      "train loss:0.011758042630246353\n",
      "train loss:0.024589820125763752\n",
      "train loss:0.017513381515432393\n",
      "train loss:0.0950666553415424\n",
      "train loss:0.055551377264116325\n",
      "train loss:0.018102174950613212\n",
      "train loss:0.06837154112082705\n",
      "train loss:0.03651745925594278\n",
      "train loss:0.028033918682569315\n",
      "train loss:0.025200458871233452\n",
      "train loss:0.045189868232675096\n",
      "train loss:0.05772603868356801\n",
      "train loss:0.015020163441761866\n",
      "train loss:0.015766314359856117\n",
      "train loss:0.010341008003450787\n",
      "train loss:0.053722715995284186\n",
      "train loss:0.0710423028886982\n",
      "train loss:0.030326326784623478\n",
      "train loss:0.009783030939278223\n",
      "train loss:0.015992791280950776\n",
      "train loss:0.01040684362701555\n",
      "train loss:0.05923729942026117\n",
      "train loss:0.0476491589815937\n",
      "train loss:0.040911837975356705\n",
      "train loss:0.0683946163359857\n",
      "train loss:0.037134685225071115\n",
      "train loss:0.03970275813240785\n",
      "train loss:0.08468158350483058\n",
      "train loss:0.013795591770344144\n",
      "train loss:0.034478448478155864\n",
      "train loss:0.019848393970513287\n",
      "train loss:0.05196501678467775\n",
      "train loss:0.048645716130862375\n",
      "train loss:0.04045829168626339\n",
      "train loss:0.051740209443331776\n",
      "train loss:0.009971939000056312\n",
      "train loss:0.014496806674127836\n",
      "train loss:0.01255927575126365\n",
      "train loss:0.030163016714752033\n",
      "train loss:0.028383683180339327\n",
      "train loss:0.0529693759157692\n",
      "train loss:0.036347939913832235\n",
      "train loss:0.02410421250932176\n",
      "train loss:0.013727176148064844\n",
      "train loss:0.05471666393421686\n",
      "train loss:0.041144832816229535\n",
      "train loss:0.027514895329104182\n",
      "train loss:0.030684743947623647\n",
      "train loss:0.07245115059632576\n",
      "train loss:0.02165621952492955\n",
      "train loss:0.07419731117541617\n",
      "train loss:0.014214374743196014\n",
      "train loss:0.010593153638616015\n",
      "train loss:0.03923521718214156\n",
      "train loss:0.07992943743235278\n",
      "train loss:0.011244661885386833\n",
      "train loss:0.021208991623403072\n",
      "train loss:0.05953179063500939\n",
      "train loss:0.06544680374334876\n",
      "train loss:0.028233352483508004\n",
      "train loss:0.051352268786273136\n",
      "train loss:0.033137317496186154\n",
      "train loss:0.034993823757719755\n",
      "train loss:0.053007730364024425\n",
      "train loss:0.08355674199238768\n",
      "train loss:0.013658560545272853\n",
      "train loss:0.03142331111114149\n",
      "train loss:0.010041016749517953\n",
      "train loss:0.0811289191693559\n",
      "train loss:0.011911842603674363\n",
      "train loss:0.022340233806295386\n",
      "train loss:0.055165035954730886\n",
      "train loss:0.02541908423430804\n",
      "train loss:0.028178917707062854\n",
      "train loss:0.027706141661758096\n",
      "train loss:0.020376531498051714\n",
      "train loss:0.014297515530948102\n",
      "train loss:0.02831132371927025\n",
      "train loss:0.03991662225278463\n",
      "train loss:0.03704760318084835\n",
      "train loss:0.042991743052186664\n",
      "train loss:0.005345825030908533\n",
      "train loss:0.016153743941934747\n",
      "train loss:0.017896732010896935\n",
      "train loss:0.07886083059519083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03275045984296547\n",
      "train loss:0.0361110793151705\n",
      "train loss:0.014519757319493067\n",
      "train loss:0.03053767775459583\n",
      "train loss:0.03470196239521489\n",
      "train loss:0.03337629289499274\n",
      "train loss:0.05631366640099508\n",
      "train loss:0.01325204695050987\n",
      "train loss:0.01337772355293179\n",
      "train loss:0.008685327606645692\n",
      "train loss:0.03076364585451222\n",
      "train loss:0.07712780824405077\n",
      "train loss:0.014627312876736126\n",
      "train loss:0.07058133085776741\n",
      "train loss:0.06345164672132446\n",
      "train loss:0.06383370298814194\n",
      "train loss:0.005504302151280584\n",
      "train loss:0.005282200297615408\n",
      "train loss:0.05122406865058414\n",
      "train loss:0.00940161016914101\n",
      "=== epoch:5, train acc:0.982, test acc:0.986 ===\n",
      "train loss:0.05467645475828692\n",
      "train loss:0.040367018058968515\n",
      "train loss:0.012442040033686841\n",
      "train loss:0.028351738165052206\n",
      "train loss:0.06414216655204984\n",
      "train loss:0.01888051732656607\n",
      "train loss:0.01606174998275349\n",
      "train loss:0.033555906272881256\n",
      "train loss:0.07762431009654552\n",
      "train loss:0.06978253045423628\n",
      "train loss:0.027909247646766243\n",
      "train loss:0.008454185854446959\n",
      "train loss:0.03195898661455108\n",
      "train loss:0.03053802034477371\n",
      "train loss:0.017558277388820785\n",
      "train loss:0.020217107155957627\n",
      "train loss:0.041246331570188755\n",
      "train loss:0.08413580594310774\n",
      "train loss:0.020081173115378537\n",
      "train loss:0.04038129060333315\n",
      "train loss:0.0184488637158388\n",
      "train loss:0.02357041882407607\n",
      "train loss:0.013850275916369972\n",
      "train loss:0.06666293981343684\n",
      "train loss:0.030156236985718992\n",
      "train loss:0.10121202939800025\n",
      "train loss:0.026374475830391816\n",
      "train loss:0.02233227045358318\n",
      "train loss:0.026960052748824258\n",
      "train loss:0.03452441176503222\n",
      "train loss:0.11323749133352702\n",
      "train loss:0.03419769418959001\n",
      "train loss:0.029356198318688922\n",
      "train loss:0.02254146795822256\n",
      "train loss:0.026780853738117692\n",
      "train loss:0.015363782146155002\n",
      "train loss:0.08200968229167843\n",
      "train loss:0.03691935568950985\n",
      "train loss:0.04840409362491549\n",
      "train loss:0.03080109155883182\n",
      "train loss:0.05027268480892892\n",
      "train loss:0.0063978342408433555\n",
      "train loss:0.00967890674431409\n",
      "train loss:0.04529370530511361\n",
      "train loss:0.02967916406575521\n",
      "train loss:0.0266844274898179\n",
      "train loss:0.009908979861230238\n",
      "train loss:0.015501824194398106\n",
      "train loss:0.023194757726868683\n",
      "train loss:0.011814892236856299\n",
      "train loss:0.018350083193066427\n",
      "train loss:0.007403494605551861\n",
      "train loss:0.030836934046384656\n",
      "train loss:0.019422482590997\n",
      "train loss:0.03386534075129811\n",
      "train loss:0.045541586262273545\n",
      "train loss:0.016966091619625045\n",
      "train loss:0.01718455368997693\n",
      "train loss:0.09126337732585733\n",
      "train loss:0.048415421041031914\n",
      "train loss:0.019765780669520512\n",
      "train loss:0.023457406797937702\n",
      "train loss:0.014058972640219485\n",
      "train loss:0.05796365821936307\n",
      "train loss:0.040457950856713804\n",
      "train loss:0.01472204034119706\n",
      "train loss:0.07493632540524438\n",
      "train loss:0.01243144644880871\n",
      "train loss:0.0562234243917946\n",
      "train loss:0.01669769506464551\n",
      "train loss:0.00896860357084748\n",
      "train loss:0.029510960089658967\n",
      "train loss:0.0711384544703897\n",
      "train loss:0.07575973789844777\n",
      "train loss:0.014261018915230666\n",
      "train loss:0.011033773529064917\n",
      "train loss:0.025125142985693173\n",
      "train loss:0.03311943755369215\n",
      "train loss:0.005706454270028264\n",
      "train loss:0.022729795573883403\n",
      "train loss:0.06514317576490714\n",
      "train loss:0.086544614649789\n",
      "train loss:0.022169263449916224\n",
      "train loss:0.003144237666120084\n",
      "train loss:0.004892783152177903\n",
      "train loss:0.03962827796611026\n",
      "train loss:0.009991568127453035\n",
      "train loss:0.027475107593655127\n",
      "train loss:0.02162283283322617\n",
      "train loss:0.023196877130355658\n",
      "train loss:0.014088136914997327\n",
      "train loss:0.009988049195516902\n",
      "train loss:0.013504128891906619\n",
      "train loss:0.084489073556063\n",
      "train loss:0.0777934783758674\n",
      "train loss:0.03970996074039195\n",
      "train loss:0.009689758549138392\n",
      "train loss:0.11183073424417588\n",
      "train loss:0.012092320901142577\n",
      "train loss:0.06827419435802637\n",
      "train loss:0.011422152464163959\n",
      "train loss:0.04052424409821406\n",
      "train loss:0.013514764811429367\n",
      "train loss:0.11332106502235899\n",
      "train loss:0.050723622433985455\n",
      "train loss:0.015586563705317557\n",
      "train loss:0.08439998767627152\n",
      "train loss:0.024541816052073607\n",
      "train loss:0.028267521394861307\n",
      "train loss:0.03408599672858385\n",
      "train loss:0.01216685993482854\n",
      "train loss:0.006824891725466857\n",
      "train loss:0.0469514051655802\n",
      "train loss:0.04878373369628835\n",
      "train loss:0.015757497488633157\n",
      "train loss:0.022097875968795977\n",
      "train loss:0.016228796327558595\n",
      "train loss:0.03938872793565835\n",
      "train loss:0.025869634023881632\n",
      "train loss:0.03095698818724819\n",
      "train loss:0.05276511260263638\n",
      "train loss:0.06326434350504213\n",
      "train loss:0.028014975372244545\n",
      "train loss:0.04757020386472832\n",
      "train loss:0.03130267682986197\n",
      "train loss:0.013300044701658315\n",
      "train loss:0.018604932960693356\n",
      "train loss:0.004874021239514386\n",
      "train loss:0.013992802380105647\n",
      "train loss:0.01039868871846087\n",
      "train loss:0.059572979742397204\n",
      "train loss:0.05756496913569313\n",
      "train loss:0.006144494106060678\n",
      "train loss:0.0050438572585100845\n",
      "train loss:0.044481224025450176\n",
      "train loss:0.010915507678540768\n",
      "train loss:0.011421097075641278\n",
      "train loss:0.02063947223493001\n",
      "train loss:0.031703776512960614\n",
      "train loss:0.04358783093265753\n",
      "train loss:0.018049568363692977\n",
      "train loss:0.0964427626289193\n",
      "train loss:0.007657036589021656\n",
      "train loss:0.024274391966132253\n",
      "train loss:0.018302860736042283\n",
      "train loss:0.050830781600854774\n",
      "train loss:0.021254102790302324\n",
      "train loss:0.015463165038192742\n",
      "train loss:0.050049056962595505\n",
      "train loss:0.04635705474825778\n",
      "train loss:0.03524072010598222\n",
      "train loss:0.031239719983771726\n",
      "train loss:0.04143358003441691\n",
      "train loss:0.014855487881622666\n",
      "train loss:0.0353650044352423\n",
      "train loss:0.015251832504966643\n",
      "train loss:0.013393221224846867\n",
      "train loss:0.03346700650247778\n",
      "train loss:0.0349738864447144\n",
      "train loss:0.09076091440836077\n",
      "train loss:0.03325022064135426\n",
      "train loss:0.05464038782866409\n",
      "train loss:0.020620220324673186\n",
      "train loss:0.1152695721563442\n",
      "train loss:0.016137357283544394\n",
      "train loss:0.03385882155152061\n",
      "train loss:0.05967443842758313\n",
      "train loss:0.04017435269665143\n",
      "train loss:0.01491682590214658\n",
      "train loss:0.016680651833385105\n",
      "train loss:0.018231458185770426\n",
      "train loss:0.0152193377717827\n",
      "train loss:0.023893865410693667\n",
      "train loss:0.017612727898313894\n",
      "train loss:0.02675597435864317\n",
      "train loss:0.005281321847606665\n",
      "train loss:0.006200096407230992\n",
      "train loss:0.0331684230495662\n",
      "train loss:0.017792895125435495\n",
      "train loss:0.03200880203068079\n",
      "train loss:0.019428388657737116\n",
      "train loss:0.08471571725320272\n",
      "train loss:0.058768913203394085\n",
      "train loss:0.0021599166324086003\n",
      "train loss:0.05868564306152622\n",
      "train loss:0.0070628284278398965\n",
      "train loss:0.03392817315563611\n",
      "train loss:0.03791297007065922\n",
      "train loss:0.035134117921017165\n",
      "train loss:0.02615227774828912\n",
      "train loss:0.059022851144038775\n",
      "train loss:0.008396828937092472\n",
      "train loss:0.01913324428222117\n",
      "train loss:0.02431785081142895\n",
      "train loss:0.019074245041589392\n",
      "train loss:0.005486747043985737\n",
      "train loss:0.014613019490210967\n",
      "train loss:0.014221937787428916\n",
      "train loss:0.029051197671841823\n",
      "train loss:0.00981162994356532\n",
      "train loss:0.007060542555225621\n",
      "train loss:0.02077013834740801\n",
      "train loss:0.026144975238181968\n",
      "train loss:0.020402987604020352\n",
      "train loss:0.09599172920331378\n",
      "train loss:0.027073282555685422\n",
      "train loss:0.06932371480685945\n",
      "train loss:0.10335045654624411\n",
      "train loss:0.012270336168183594\n",
      "train loss:0.017533770048272872\n",
      "train loss:0.005939017094326132\n",
      "train loss:0.04007166244077851\n",
      "train loss:0.01467493245217462\n",
      "train loss:0.011736661957497332\n",
      "train loss:0.017993807356278167\n",
      "train loss:0.0072422243911342684\n",
      "train loss:0.007128387182712407\n",
      "train loss:0.021431095260869983\n",
      "train loss:0.010180408024502256\n",
      "train loss:0.02023132852343644\n",
      "train loss:0.05021547769445252\n",
      "train loss:0.01904355823771771\n",
      "train loss:0.07513982500532118\n",
      "train loss:0.07157656954686939\n",
      "train loss:0.04480060764253443\n",
      "train loss:0.24406807267641956\n",
      "train loss:0.026594595127096162\n",
      "train loss:0.04090792912537756\n",
      "train loss:0.01152451246594997\n",
      "train loss:0.05461900670000738\n",
      "train loss:0.01783113035053987\n",
      "train loss:0.026491051833608847\n",
      "train loss:0.010364134989602515\n",
      "train loss:0.010115147061322377\n",
      "train loss:0.019155284604093093\n",
      "train loss:0.05600382835921711\n",
      "train loss:0.0072667123002100095\n",
      "train loss:0.006470163058025911\n",
      "train loss:0.08173041497119411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06772326995398859\n",
      "train loss:0.022425655662035134\n",
      "train loss:0.01091953148769212\n",
      "train loss:0.04175731644072644\n",
      "train loss:0.03859341707108091\n",
      "train loss:0.08353490427550955\n",
      "train loss:0.028030556524832276\n",
      "train loss:0.006135412082560722\n",
      "train loss:0.06252125250266637\n",
      "train loss:0.07462073803311822\n",
      "train loss:0.014774718355042247\n",
      "train loss:0.010434840578327753\n",
      "train loss:0.027432352758243083\n",
      "train loss:0.044113333266113726\n",
      "train loss:0.015395576914380068\n",
      "train loss:0.013841407928539176\n",
      "train loss:0.03900100730873162\n",
      "train loss:0.025697380603552908\n",
      "train loss:0.013628141312308141\n",
      "train loss:0.030503300453370966\n",
      "train loss:0.02042639621963207\n",
      "train loss:0.019745451225736087\n",
      "train loss:0.03414045488052784\n",
      "train loss:0.02391077403628676\n",
      "train loss:0.030313694522137148\n",
      "train loss:0.034164451588622094\n",
      "train loss:0.01055452805247105\n",
      "train loss:0.010748678106120678\n",
      "train loss:0.01929675713495054\n",
      "train loss:0.009564109593176854\n",
      "train loss:0.01304187177084342\n",
      "train loss:0.04782593801021307\n",
      "train loss:0.032579030617656796\n",
      "train loss:0.011229399205887343\n",
      "train loss:0.026556344493929632\n",
      "train loss:0.01679285561224471\n",
      "train loss:0.01648419102142324\n",
      "train loss:0.060713585953525724\n",
      "train loss:0.021945717156775225\n",
      "train loss:0.01420335230747281\n",
      "train loss:0.0692641467870574\n",
      "train loss:0.03216315122416541\n",
      "train loss:0.01768267350738608\n",
      "train loss:0.01867805602005278\n",
      "train loss:0.007168648215647579\n",
      "train loss:0.04507840709176966\n",
      "train loss:0.024875650394131376\n",
      "train loss:0.01786996740195404\n",
      "train loss:0.05595298633637013\n",
      "train loss:0.025340020559155418\n",
      "train loss:0.014901818376400396\n",
      "train loss:0.0063737806250926685\n",
      "train loss:0.006601429554678815\n",
      "train loss:0.026799543149685582\n",
      "train loss:0.024564964176671658\n",
      "train loss:0.04471496892813034\n",
      "train loss:0.018071186173867028\n",
      "train loss:0.021567873187629042\n",
      "train loss:0.009396505329619638\n",
      "train loss:0.015960433019037563\n",
      "train loss:0.026692104432255667\n",
      "train loss:0.020191510784834374\n",
      "train loss:0.007703474265276361\n",
      "train loss:0.013266924977579041\n",
      "train loss:0.005102591998403067\n",
      "train loss:0.0174260317208237\n",
      "train loss:0.007332200657213368\n",
      "train loss:0.05034981010836669\n",
      "train loss:0.03764934827042418\n",
      "train loss:0.01875981859818475\n",
      "train loss:0.0035687704267176167\n",
      "train loss:0.013476869249200387\n",
      "train loss:0.024911829643753335\n",
      "train loss:0.027362560806008854\n",
      "train loss:0.017285879757372884\n",
      "train loss:0.04393392686918052\n",
      "train loss:0.023049491769814856\n",
      "train loss:0.03419791166517131\n",
      "train loss:0.014356889821049679\n",
      "train loss:0.05066878689282875\n",
      "train loss:0.013192858934399108\n",
      "train loss:0.0169795653114992\n",
      "train loss:0.004606438048439911\n",
      "train loss:0.05767986443602023\n",
      "train loss:0.049979427664807605\n",
      "train loss:0.04534504010243446\n",
      "train loss:0.030963004229323025\n",
      "train loss:0.01047149051687738\n",
      "train loss:0.012959814183259413\n",
      "train loss:0.006197699016075373\n",
      "train loss:0.007493037421353203\n",
      "train loss:0.06824313831586877\n",
      "train loss:0.02416537396398723\n",
      "train loss:0.010168225011076564\n",
      "train loss:0.01589302201695953\n",
      "train loss:0.034789869281799564\n",
      "train loss:0.04633536777188508\n",
      "train loss:0.009739952527220456\n",
      "train loss:0.01229688560141792\n",
      "train loss:0.013384569336172398\n",
      "train loss:0.05220091200857199\n",
      "train loss:0.017625811661125167\n",
      "train loss:0.009775831049536957\n",
      "train loss:0.06130395309268387\n",
      "train loss:0.025775510723094027\n",
      "train loss:0.008360691438510514\n",
      "train loss:0.028089985186624023\n",
      "train loss:0.006348327978876982\n",
      "train loss:0.012050381829720696\n",
      "train loss:0.058534049571648594\n",
      "train loss:0.05072517815764519\n",
      "train loss:0.017933015039656833\n",
      "train loss:0.027950107945053034\n",
      "train loss:0.03928178605487527\n",
      "train loss:0.03976500291656764\n",
      "train loss:0.010932934937822762\n",
      "train loss:0.05112508250032215\n",
      "train loss:0.014112561501700125\n",
      "train loss:0.023450870471144934\n",
      "train loss:0.01620678399135994\n",
      "train loss:0.01168567855469441\n",
      "train loss:0.009216576661956452\n",
      "train loss:0.012571665363753473\n",
      "train loss:0.0324078482655892\n",
      "train loss:0.02402870955115756\n",
      "train loss:0.025726302118770077\n",
      "train loss:0.05912864201454828\n",
      "train loss:0.02534298470076799\n",
      "train loss:0.019182830728982764\n",
      "train loss:0.04847787361642325\n",
      "train loss:0.01730532864027846\n",
      "train loss:0.04960907426603114\n",
      "train loss:0.005931561786341469\n",
      "train loss:0.015322252374129052\n",
      "train loss:0.014928470540524883\n",
      "train loss:0.027686145692620703\n",
      "train loss:0.05450124292501068\n",
      "train loss:0.042722994982263236\n",
      "train loss:0.014639803817822825\n",
      "train loss:0.031946160375998074\n",
      "train loss:0.021833438040330822\n",
      "train loss:0.05208003912678277\n",
      "train loss:0.008550495261790917\n",
      "train loss:0.006241169021702986\n",
      "train loss:0.004655668534109828\n",
      "train loss:0.018594022051559717\n",
      "train loss:0.023188906104089888\n",
      "train loss:0.012528461670037142\n",
      "train loss:0.006884036295117753\n",
      "train loss:0.014251874658717332\n",
      "train loss:0.023387305217188955\n",
      "train loss:0.051068200196641565\n",
      "train loss:0.08522050040932878\n",
      "train loss:0.02204365374324151\n",
      "train loss:0.02696987534756401\n",
      "train loss:0.007177719589239941\n",
      "train loss:0.04733964346342378\n",
      "train loss:0.05183382274663928\n",
      "train loss:0.01241166609293027\n",
      "train loss:0.006990655883846884\n",
      "train loss:0.01190817039289981\n",
      "train loss:0.02619588960510128\n",
      "train loss:0.018313256502678215\n",
      "train loss:0.03878254487638312\n",
      "train loss:0.00831961947679263\n",
      "train loss:0.020817370714553047\n",
      "train loss:0.01487134019574011\n",
      "train loss:0.017549737231086156\n",
      "train loss:0.03432018675357302\n",
      "train loss:0.01702118679112699\n",
      "train loss:0.012228988250825946\n",
      "train loss:0.04307519628584389\n",
      "train loss:0.0035031067975391066\n",
      "train loss:0.05554420000817804\n",
      "train loss:0.036096184755073105\n",
      "train loss:0.01658355108019476\n",
      "train loss:0.010279560114222685\n",
      "train loss:0.010276694699362788\n",
      "train loss:0.004797621835528759\n",
      "train loss:0.0027198412010087407\n",
      "train loss:0.041353623832216385\n",
      "train loss:0.019753656149082286\n",
      "train loss:0.006555812151583249\n",
      "train loss:0.02968329612035183\n",
      "train loss:0.052492950405206476\n",
      "train loss:0.023475625217654058\n",
      "train loss:0.05631767588783252\n",
      "train loss:0.01492673939597003\n",
      "train loss:0.02731146990238417\n",
      "train loss:0.055219486044043906\n",
      "train loss:0.01966955113162174\n",
      "train loss:0.011234237423877176\n",
      "train loss:0.004399426616480629\n",
      "train loss:0.022255487194144587\n",
      "train loss:0.08651180046549374\n",
      "train loss:0.03237813823752742\n",
      "train loss:0.031575155939191156\n",
      "train loss:0.07482845732065907\n",
      "train loss:0.013390151991523476\n",
      "train loss:0.016749708292845455\n",
      "train loss:0.020603883001008412\n",
      "train loss:0.009018539339398174\n",
      "train loss:0.02315393508681984\n",
      "train loss:0.025852358199868336\n",
      "train loss:0.010796869516479925\n",
      "train loss:0.005432542628385919\n",
      "train loss:0.0098919296516098\n",
      "train loss:0.0104648930668928\n",
      "train loss:0.00799091893106069\n",
      "train loss:0.01258721700210749\n",
      "train loss:0.07066097263400421\n",
      "train loss:0.00854907134744959\n",
      "train loss:0.016347101575899445\n",
      "train loss:0.00566009023622657\n",
      "train loss:0.033409657097492204\n",
      "train loss:0.004015982893053301\n",
      "train loss:0.01803306278504082\n",
      "train loss:0.02831258242328811\n",
      "train loss:0.04526194524241767\n",
      "train loss:0.015132078466464251\n",
      "train loss:0.0431103037794013\n",
      "train loss:0.08498646984598729\n",
      "train loss:0.008440537105069017\n",
      "train loss:0.029364132926669444\n",
      "train loss:0.005060183926586717\n",
      "train loss:0.051155625223392585\n",
      "train loss:0.043549324726924014\n",
      "train loss:0.025915037041029567\n",
      "train loss:0.014630208108707759\n",
      "train loss:0.04877423808786709\n",
      "train loss:0.012659714142930217\n",
      "train loss:0.043652029032303336\n",
      "train loss:0.01824151873389452\n",
      "train loss:0.04195702161508694\n",
      "train loss:0.011363894501262285\n",
      "train loss:0.02613443627339464\n",
      "train loss:0.029296740304453645\n",
      "train loss:0.02867474368625172\n",
      "train loss:0.0321418389224333\n",
      "train loss:0.013304494508401033\n",
      "train loss:0.03014654200236884\n",
      "train loss:0.016499035573659324\n",
      "train loss:0.006442154790682589\n",
      "train loss:0.05839045746546156\n",
      "train loss:0.01639897692892917\n",
      "train loss:0.03173796755093968\n",
      "train loss:0.06116530821702896\n",
      "train loss:0.009534190939380242\n",
      "train loss:0.045477717873770016\n",
      "train loss:0.03673430562570841\n",
      "train loss:0.04156916970647998\n",
      "train loss:0.01390206127234907\n",
      "train loss:0.03343504384005993\n",
      "train loss:0.007943425541943475\n",
      "train loss:0.03305774356739728\n",
      "train loss:0.013574530358071206\n",
      "train loss:0.00576423521710394\n",
      "train loss:0.06529267188346878\n",
      "train loss:0.04244740366007545\n",
      "train loss:0.05606503659488674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.008250314040614866\n",
      "train loss:0.02244185876494965\n",
      "train loss:0.05035624870212763\n",
      "train loss:0.01934217848257812\n",
      "train loss:0.012297805881249967\n",
      "train loss:0.032265300598761794\n",
      "train loss:0.0082110951597341\n",
      "train loss:0.005126181638588202\n",
      "train loss:0.015649817594326217\n",
      "train loss:0.04149787468473061\n",
      "train loss:0.0357008515370824\n",
      "train loss:0.017308274924504135\n",
      "train loss:0.007314690979395759\n",
      "train loss:0.06310852604359868\n",
      "train loss:0.048269140142712094\n",
      "train loss:0.03825885612636295\n",
      "train loss:0.06856493294261878\n",
      "train loss:0.11322249694122291\n",
      "train loss:0.03247606023368574\n",
      "train loss:0.04923011866922797\n",
      "train loss:0.018913160876100496\n",
      "train loss:0.018717483045354234\n",
      "train loss:0.02304895684792696\n",
      "train loss:0.049163085407353965\n",
      "train loss:0.014660604505376906\n",
      "train loss:0.031796194667962935\n",
      "train loss:0.005571954465706167\n",
      "train loss:0.0039380793199086\n",
      "train loss:0.027068633425785246\n",
      "train loss:0.01329082024353206\n",
      "train loss:0.021155635316192654\n",
      "train loss:0.043771383235615184\n",
      "train loss:0.02781822567090905\n",
      "train loss:0.052892076978020314\n",
      "train loss:0.02007617607039336\n",
      "train loss:0.019378149007818823\n",
      "train loss:0.025070966651190966\n",
      "train loss:0.03491388328484796\n",
      "train loss:0.04982247653733775\n",
      "train loss:0.0071979877011706704\n",
      "train loss:0.058712318952373416\n",
      "train loss:0.09381609734625179\n",
      "train loss:0.023554153744971284\n",
      "train loss:0.0240491375053589\n",
      "train loss:0.011497595560893421\n",
      "train loss:0.014573079565439595\n",
      "train loss:0.02596021172372971\n",
      "train loss:0.03132311777235203\n",
      "train loss:0.0443192011491639\n",
      "train loss:0.04930834053550081\n",
      "train loss:0.008380098555671603\n",
      "train loss:0.06294338717649098\n",
      "train loss:0.04109974603114929\n",
      "train loss:0.009515123951251175\n",
      "train loss:0.03155873180350573\n",
      "train loss:0.00688642882215194\n",
      "train loss:0.009251153073754147\n",
      "train loss:0.06482432392921718\n",
      "train loss:0.0077763164865412146\n",
      "train loss:0.057526896179776604\n",
      "train loss:0.027635141055923094\n",
      "train loss:0.008383362421651134\n",
      "train loss:0.003843291051794718\n",
      "train loss:0.01646061891071812\n",
      "train loss:0.047164933202214814\n",
      "train loss:0.09547263500836545\n",
      "train loss:0.04679978188310639\n",
      "train loss:0.006348393101452284\n",
      "train loss:0.01069943795981232\n",
      "train loss:0.020347977533585148\n",
      "train loss:0.02018015175956351\n",
      "train loss:0.04391731608112487\n",
      "train loss:0.01117286280056466\n",
      "train loss:0.009535101378065082\n",
      "train loss:0.020306384258046095\n",
      "train loss:0.02104824059546709\n",
      "train loss:0.0313279916612276\n",
      "train loss:0.05255375105996087\n",
      "train loss:0.007943320157012216\n",
      "train loss:0.007056358551773642\n",
      "train loss:0.03324812705250028\n",
      "train loss:0.014342783647967256\n",
      "train loss:0.014904368578773483\n",
      "train loss:0.0493001330692373\n",
      "train loss:0.003913842261778554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a446449b0e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# パラメータの保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Deep_Learning/common/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Deep_Learning/common/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-55e773cb88d7>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-55e773cb88d7>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-55e773cb88d7>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Deep_Learning/common/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Deep_Learning/common/util.py\u001b[0m in \u001b[0;36mim2col\u001b[0;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`pad_width` must be of integral type.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m     \u001b[0mnarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m     \u001b[0mpad_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
